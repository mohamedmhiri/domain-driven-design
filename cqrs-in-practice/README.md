Course Overview
Course Overview

(Music) Hi, everyone. My name is Vladimir Khorikov, and welcome to my course, CQRS in Practice. I'm a Domain-Driven Design evangelist and very excited to share this course with you. The CQRS pattern has become quite well known in the sphere of Domain-Driven Design; however, there are still a lot of misconceptions around this pattern, especially when it comes to applying it in the real-world software projects. Should you use it with event sourcing? Should you always have separate databases for reads and writes? How to implement command handlers using the modern frameworks such as ASP. NET Core. The course will answer all these questions and more. Some of the major topics that we will cover include: refactoring towards task-based interface and away from the crowd-based thinking, implementing command and query handler decorators, extracting separate data storage for reads, and common best practices and misconceptions around CQRS. By the end of this course, you will know everything needed to start implementing the CQRS pattern in your own projects. Before beginning this course, you should be familiar with the C# programming language. I hope you will join me on this journey to learn CQRS here at Pluralsight.
Introduction
Introduction

Hi, my name is Vladimir Khorikov and this is the CQRS in Practice course. If you are familiar with Domain-Driven Design, you've most likely heard about CQRS, which stands for Command-Query Responsibility Segregation. In fact, this pattern has become almost as well-known as the concept of Domain-Driven Design itself; however, there are still a lot of misconceptions around this pattern, especially when it comes to applying it in the real-world software projects. Should you use it with event sourcing? Should you always have separate databases for reads and writes? Should you make the synchronization between reads and writes asynchronous? And so on and so forth. This course will answer all of these questions and more. You will learn exactly what CQRS is, the principles behind it, and the benefits it can provide for your project. You will also learn about all the common misconceptions and anti-patterns around it. You will see a detailed, step-by-step process of implementing this pattern in practice. The sample project we'll be working on is close to what you can see in the real world, and I will explain each step on the way to CQRS in great detail. Here's a quick outline of this course. In the first module, we will talk about what CQRS is, its origin, and the benefits it can provide for your project. In the next module, I will introduce an online student management system implemented without the CQRS pattern in mind. You will see firsthand how the CRUD-based API reduces the quality of one's code base and damages the user experience. In the third module, we will start the refactoring. We will segregate commands from queries by introducing two models where previously there was only one. You will see how it allows us to offload the read operations from the domain model and thus make this model simpler. Next, you will learn what a CRUD-based interface is and how to refactor it towards a task-based one. In the fifth module, we will simplify the read model. We will achieve that by bypassing the domain model and the ORM when reading data from the database. This will allow us to optimize the performance of reads in the application. In the next module, we will look at MediatR, an open-source library that was built specifically with CQRS in mind and perfectly fits this pattern. In the seventh module, we are going to introduce a separate database for queries, and in Module 8, implement synchronization between the two. Finally, in the last module, we will talk about CQRS best practices and misconceptions. You will learn about common questions people ask when starting out with CQRS, such as, for example, the distinction from event sourcing. During the course, you will see a lot of refactoring techniques, which I'll explain in detail as we go through them. For this course, you will need a basic knowledge of what Domain-Driven Design is. I recommend my Domain-Driven Design in Practice course to obtain this knowledge.
CQRS and Its Origins

CQRS stands for Command-Query Responsibility Segregation. The idea behind this pattern is extremely simple. Instead of having one unified model, you need to introduce two: one for reads and the other one for writes, and that's basically it. Despite its simplicity, however, this simple guideline leads to some significant benefits. We will cover all of them in the subsequent modules. For now, let's elaborate on this basic idea, and talk about the origin of CQRS. CQRS was introduced by Greg Young back in 2010. Here's the book about CQRS he wrote in the same year. Greg, himself, based this idea on the command-query separation principle coined by Bertrand Meyer. Command-query separation principle, CQS for short, states that every method should either be a command that performs an action, or a query that returns data to the caller, but not both. In other words, asking a question should not change the answer. More formally, methods should return a value only if they are referentially transparent and don't incur any side effects, such as, for example, mutating the state of an object, changing a file in the file system, and so on. To follow this principle, you need to make sure that if a method changes some piece of state, this method should always be of type void, otherwise, it should return something. This allows you to increase the readability of your code base. Now you can tell the method's purpose just by looking at its signature. No need to dive into its implementation details. Note that it is not always possible to follow the command-query separation principle and there almost always will be situations where it would make more sense for a method to both have a side effect and return something. An example here is Stack. Its Pop method removes the element pushed into the stack last and returns it to the caller. This method violates the CQS principle, but at the same time, it doesn't make a lot of sense to separate those responsibilities into two different functions. Other examples include situations where the result of a query can become stale quickly, and so you have to join the query with the command. In this case, you both perform the operation and return the result of it. For example, here, on the left, you can see two methods, one for writing to a file, and the other one for ensuring that this file exists. The idea here is that before calling this method, the client code needs to make sure the file exists by using this one. The FileExists method is a query here. It returns a Boolean value and doesn't mutate the file, and WriteToFile is a command. It changes the file and its return type is void. So, these two methods follow the CQS principle. However, there's a problem with this code. The result of the query can become stale by the time the client code invokes the command. There could be some other process intervening right between these two calls, and it can delete the file after the query is called, but before we invoke the command, and so to avoid this problem, we have to violate the command-query separation principle and come up with a new version of the method, like this one. As you can see, instead of checking for the file existence, it tries to update it, and if there's no such file, it gets an exception, catches it, and returns a failed result. The operation is atomic now. That's how we avoid information staleness. The downside here is that this method no longer follows the CQS principle. Other examples where the command-query separation principle is not applicable involve multi-threaded environments where you also need to ensure that the operation is atomic. However, it's still a good idea to make the CQS principle your default choice, and depart from it only in exceptional cases, like those I described above. So what's the relation between CQS and CQRS? CQRS takes this same idea and extends it to a higher level. Instead of methods like in CQS, CQRS focuses on the model and classes in that model, and then applies the same principles to them. Just like CQS encourages you to split a method into two, a command and a query, CQRS encourages you to untangle a single, unified domain model and create two models: one for handling commands or writes, and the other one for handling queries, reads. Like I said, the principle is extremely simple. However, it entails very interesting consequences. So let's discuss them next.
Why CQRS?

Alright, so CQRS is about splitting a single model into two; one for reads and the other one for writes, but of course, this is not the end goal in and of itself. So what is it then? What are the benefits the CQRS pattern provides? First of all, it's scalability. If you look at a typical enterprise level application, you may notice that among all operations with this application, among all those create, read, update, and delete operations, the one that is used the most is usually read. There are disproportionately more reads than writes in a typical system, and so it's important to be able to scale them independently from each other. For example, you can host the command side on a single server, but create a cluster of 10 servers for the queries. Because the processing of commands and queries is fundamentally asymmetrical, scaling these services asymmetrically makes a lot of sense, too. Secondly, it's performance. This one is related to scalability, but it's not the same thing. Even if you decide to host reads and writes on the same server, you can still apply optimization techniques that wouldn't be possible with a single unified model. For example, just having a separate set of APIs for queries allows you to set up a cache for that specific part of the application. It also allows you to use database-specific features and hand-crafted, highly sophisticated SQL for reading data from the database without looking back at the command side of the application where you probably use some kind of ORM, but probably the most significant benefit here is simplicity. The command side and the query side have drastically different needs, and trying to come up with a unified model for these needs is like trying to fit a square peg in a round hole. The result always turns out to be a convoluted and over-complicated model that handles neither of these two parts well. And often, the very act of admitting that there are two different use cases involved allows you to look at your application in a new light. By making the difference between them explicit and introducing two models instead of just one, you can offload a lot of complexity from your code base. Now all of the sudden, you don't need to worry about handling two completely different use cases with the same code. You can focus on each of them independently and introduce a separate solution that makes the most sense in each particular case. You can view this as the single responsibility principle applied at the architectural level. In the end, you get two models, each of which does only one thing, and does it well. So, to summarize, we can say that CQRS is about optimizing decisions for different situations. You can choose different levels of consistency, different database normal forms, and even different databases themselves for the command and query sides, all because you are able to think of commands and queries and approach them independently. You will see all these benefits in action when we'll be working on the sample project.
CQRS in the Real World

You might not realize it, but you probably have already employed the CQRS pattern in one form or the other in the past. Let's look at some examples from real-world projects. If you ever used Entity Framework or NHibernate for writing data to the database, and raw SQL with plain ADO. NET for reading it back, that was CQRS right there. You probably thought at the moment that the necessity to drop the ORM and resort to the bare SQL for reads is just that, the necessary evil, a trade-off you have to make in order to comply with the performance requirements, but, no. This is a perfectly legitimate pattern, CQRS that is. Also, if you ever created database views optimized for specific read use cases, that was a form of CQRS as well. Another common example is ElasticSearch or any other full-text search engine. It works by indexing data, usually from a relational database, and providing rich capabilities to query it. That's exactly what CQRS is about. You have one model for writes and a completely separate model for reads, except that in this particular case, you don't build that second model yourself, but leverage an already-existing software.
Summary

In this module, you learned that Command Query Responsibility Segregation is a pattern originating from the command-query separation principle. CQRS extends CQS to the architectural level. Just like CQS encourages you to split a method into two methods, a query and a command, CQRS encourages you to untangle a single, unified domain model and create two models: one for handling commands, and the other one for handling queries. CQRS allows us to make different decisions for reads and writes, which in turn brings three benefits: scalability, performance, and the biggest one, simplicity. You can view CQRS as the single responsibility principle applied at the architectural level. In the end, you get two models, each of which does only one thing, and does it really well. We also discussed examples of applying the CQRS pattern in the real world. ElasticSearch and database views are among them. In the next module, we will look at a sample project that's implemented without the CQRS pattern in mind. We will analyze it, discuss its drawbacks, and then start making steps towards implementing CQRS.
Introducing a Sample Project
Introduction

Hi, my name is Vladimir Khorikov and this the CQRS in Practice course. In this module, we will take a look at the sample project. I will guide you through its code base, and we will talk about its purpose and the underlying problem domain.
Problem Domain Introduction

The application we'll be working on is the API for a student management system. Here's the domain model of this system. This is the main class in our model, Student. It consists of a name, email, collection of enrollments, and a collection of disenrollment comments. We'll get back to this one in a moment. A student can have up to two enrollments in courses. Each enrollment has a grade: either A, B, C, D, or F. The courses themselves have a name and the number of credits students receive for them. Whenever the user deletes an enrollment, he or she must specify the reason why the student wants to discontinue taking this course. This is what the disenrollment class is for. It shows which course was discontinued, the date and time of when this happened, and the comment from the student. All fields are mandatory. It means that when you create an enrollment, you must specify the student's grade, and when you delete one, you have to type in the comment from the student. The database pretty much mimics this domain model. I'll show it to you shortly. The API exposes the standard CRUD operations: the standard create, read, update, and delete. Here you can see a user interface that works with our API. Note that this UI is for visualization purposes only. It's easier to follow the changes in an API project when you see how they affect the user experience. We will not be dealing with the code of the UI in this course, only with the API underneath it. Also note that although it's a desktop application, it could very well be a web one, written in JavaScript. It works with the API by parsing and sending JSON messages, just like a regular web application would. I've chosen a desktop UI because I personally have more experience with them than with web ones. Alright, so you can see here a list of students with a single student, Alice. The list shows the students' names, emails, and the courses they are enrolled in. Students can be enrolled in only two courses, and so they are displayed here right away, in separate columns. Those columns contain the name of the course, the grade the student received, and the number of credits this enrollment provided. Note that the grade is taken from the enrollment database table, whereas the number of credits comes from the course table. So this is the read operation. The application provides the create, update, and delete operations, too. Let me show them to you. When we create a student, we need to specify a name and an email. Let it be Bob and bob@outlook. com, and we can also indicate the student's enrollments. Although these fields are optional, we could leave them empty. So, let's say that Bob has chosen Literature and received an A for it, and let's also say that he's chosen Trigonometry as his second course, and had a B for it. Saving this, and you can see the new student has appeared in the list. Now as the student exists, we can update it. For example, we can change the enrollment from Literature to Calculus, the grade, and if I reopen it, you can see that the change is persisted successfully. We can also delete the enrollment. To do that, we need to select the empty option from the list of courses. When we do that, the application tells us to provide a disenrollment comment, and this comment is mandatory. If I try to save the form without it, the API returns an error, and the UI displays it in this message box. Don't want to take this class anymore, and click OK. And you can see that Bob now has only one course. Calculus has been removed from his list of enrollments. Note that although we deleted the first course, the UI shows the empty slot in the second one. That's because, from our system's perspective, there is no difference in the order in which the student enrolls in the courses. And so the course that was the second one becomes the first, after we remove the enrollment. The field for the course grade shows up automatically when I try to create a new enrollment, and it's mandatory, too. If I try to save the form without it, the API also returns an error. Alright, along with all these features, the API exposes some pretty sophisticated search functionality in here. We can ask it to return only students who are enrolled in a specific course, and we can also request students who are enrolled in a particular number of courses. For example, I can choose Calculus, and the application shows that only Alice is enrolled in it. If I select Microeconomics, no one is in here, and if I ask for students who are enrolled in exactly one course, you can see that the UI shows us Bob, and if I select two, it's Alice, just as we would expect. Finally, we can delete a student. Here I can select Bob, and click Delete. Alright, let's now look at the code.
Application Code Introduction

The source code for this course is available on GitHub. You can use this short link to access it. By the moment you see this course, there would be two folders in the source code repository: Before and After. You can follow our discussion throughout this course and refactor the application with me. To do that, just navigate to the solution in the Before folder. It contains the application in the state you will see shortly. Alright, so here it is, our code base. You can see here three projects: API, Logic, and UI. The API is an ASP. NET Core application that targets. NET Core 2. 1. Logic also targets. NET Core 2. 1, and because the UI is a desktop application written in WPF, it targets the full framework. In this folder, you can find the script that will create the database schema and data required for our sample solution. Here's the content of the API project, and the Logic one. Let's look at the domain model first. This is the course class. Pretty straightforward, as you can see; just the name and the number of credits. Its mapping is straightforward, too. I'm using NHibernate here as the ORM and Fluent NHibernate to work with NHibernate mappings in a fluent manner. And here's our main domain class, Student. It consists of name and email. It also has a collection of enrollments. Each enrollment represents a link between a student and a course, with some grade. The grade is an enum that can be either A, B, C, D, or F. Aside from constructors, there's also an Update method that updates the course the student is enrolled in and the grade. By the way, the pattern you see here is a good way to work with collections in domain classes. The IList field is what the ORM maps to when loading enrollments from the database, but instead of exposing this collection as is, you make it private and create a wrapper property around it with the type IReadOnlyList, which shows the client code that it cannot modify this collection and have to use separate methods for that. Because there can be only two enrollments per student, there are two helper properties in here: first enrollment, and second enrollment. They use this GetEnrollment method in order to return the appropriate enrollment. If there is no enrollment with such an index, the method returns null. Note that despite this invariant for having maximum of two enrollments per student, the product owner stated that this limit could be increased at some point in the future. Hence this collection of enrollments, and not just two properties here and in the database. We don't want to put ourselves in a difficult position by denormalizing the database prematurely, because it would be hard to add more enrollments in this case. Alright, there's also a collection of disenrollments here. These are, as you remember, for the comments the student provides when they disenroll from the course. Here is its content: the student, the course, the date and time, and the comment itself. Back to the Student. Here, you can see two constructors: one for the ORM, which is non-public, and the other one for the client code, which shows that in order to create a student, it needs to provide a name and an email. Note that in a real-world application, you would want to validate these two strings and probably even wrap them into value objects. I'm omitting this for brevity. Along with the above properties, we also have three public methods. RemoveEnrollment deletes an enrollment from the student, AddDisenrollmentComment is for adding a comment when the student disenrolls, and Enroll is for creating an enrollment. Note this check here. If the number of enrollments is already two or more, we throw an exception. This way, we ensure that the domain model maintains its invariants and always resides in a valid state. Here is the mapping for the student entity. Id, two properties, and two collections. Resharper tells me that this is unnecessary. Okay, this part of the mapping tells NHibernate to map directly to the backing field. The inverse setting, that this side of the relationship is not the one that determines the value of the StudentID column in the Student database table, and Cascade. AllDeleteOrphan that NHibernate needs to delete all enrollments and disenrollments from the database when we delete them from these collections. Okay, let's now review the API project. The BaseController class contains some helper methods for us to use in other controllers. For example, we can call OK with some resulting object, and this method will wrap it into an envelope before passing to the client. The envelope is a class that helps keep the structure of all responses from the server consistent. You can see that along with the result itself, it contains the error message, if any, and the time this response was generated. The non-generic envelope contains some useful helper methods. And here is the StudentController, which contains the actual application functionality. It accepts a unit of work, which is a wrapper on top of NHibernate's session and transaction, and creates two repositories. The GetList method is what returns the students to display in the data grid on the UI. It accepts two filter parameters: the name of the course the student is enrolled in, and the total number of courses. Here's the repository method that does the actual filtration. We first form an IQueryable that represents a SQL query NHibernate will generate for the database. If the course name is specified, we build up this query by adding a Where clause to it. After that, we force NHibernate to execute it and give us the resulting list of students, and then if the second filter is set, we return only students who have this exact number of courses. Alright, so after the repository returns us all those domain objects, we convert them into DTOs, data transfer objects, and return to the client. The DTO looks like this. It contains and id, name, email of the student, and it also contains the information about his two courses. The Create method creates a student. It accepts this same DTO back. It reads the name and the email from it and uses them to instantiate a student. If the first course name and its grade is set up, the API finds this course, and enrolls the student to it. Note that here we are parsing the incoming grade, which is string, into the enum, and we do the same for the second course enrollment. After that, we save the student, commit the transaction, and return OK to the client. The Delete method is pretty straightforward, too. It finds the student by the id and deletes it. The main complexity lies in the Update method. Here, we also find the student, update their name and email, and then start the synchronization process. We first determine whether there are differences between the incoming DTO and the existing first enrollment. For that, we are using this private method, HasEnrollmentChanged. If the incoming enrollment course name is empty and the existing enrollment is empty, too, we consider this as no change. If one of them is empty and the second one is not, or if the course name or grade differ, it means the user has updated the enrollment. So, after we ensured that there is indeed a change to this enrollment, we need to decide which one is that. If the incoming course name is empty, that means the student disenrolls, and we need to check that the disenrollment comment is provided. If so, we remove the enrollment from the student, and add a disenrollment comment. Next, if the incoming course is not empty, it means one of two things: either the student enrolls in a new course or updates an existing enrollment. In either case, we need to make sure that the grade is provided. After that, we get the course from the database by its name, and check to see if the existing enrollment is null. If it is, it means that the student enrolls. If not, that they update the existing enrollment, and let me actually inline this variable. There is no need to have a separate one here. And we do the same procedure for the second enrollment. Let me inline the variable here, too. Okay. After the synchronization is done, we commit the transaction, and return success to the caller. Alright, that's it for the application functionality. Let's glance over the rest of the code real quick. You saw the student DTO already. It contains all the fields required to display a student in the grid, and update it when posting the changes back to the server. ExceptionHandler is an infrastructure class that catches all unhandled exceptions and converts them into 500 responses. And here's the Startup class, and let me scroll down so that you see there's nothing else in this class. SessionFactory is a class that we use to generate NHibernate sessions. There should be only one such class in the application, hence we add it as a singleton. UnitOfWork is a wrapper on top of sessions themselves and should be instantiated and disposed of on each web request. This AddScoped method does exactly that. It creates a unit of work and because this class implements the IDisposable interface, ASP. NET disposes of it after the request is completed. And here's the underlying database. It pretty much mimics the domain model. The Student table consists of the id, name, and email. Enrollment references the student and the course, and it also has a grade which is represented as integer. Disenrollment, all the same fields as in the domain model, and the course. Alright, that's it for the application code base. Let's now talk about its drawbacks.
Application Code Drawbacks

I'm sure you have worked with such CRUD-based applications a lot in the past, so much that you might not even realize that there is something wrong with this code base. So, what is it? As we discussed in the first module, CQRS brings us three benefits: scalability, performance, and simplicity. Scalability is something we will talk about later in this course, and it's not obvious that this application suffers from scalability issues anyway, so let's skip it for now. What about performance? Well, let's open the StudentRepository once again. This is the method that filters the existing students by courses that they are enrolled in and the total number of such courses. What can you tell about its performance? Up to this moment, this method operates upon an IQueryable, which is good because all the LINQ statements made upon an IQueryable object translate into the corresponding SQL. It means that this Where statement, this filtration, is done in the database itself, which is exactly what we want. But here, we force the ORM to fetch all the students with this course into memory, and only after that, continue narrowing them down. It means that the database will send us an excessive amount of data, which we then filter manually in the memory. This is a suboptimal solution and can hit the performance quite badly. It's not noticeable in our sample application, of course, but that's because there are just a few students in our database. In a real-world project, with hundreds of thousands or even millions of records, it would. Ideally, you should transfer only the minimum amount of data between the server and its database. But why are we doing that? Why force the ORM to fetch all data from the database on this step? It's because we wouldn't be able to do this filtration otherwise. Neither NHibernate nor Entity Framework support filtering by the number of elements in a one-to-many relationship, and so the only possible solution here is to finish up this filtration in the memory, using the in-memory LINQ provider. Another performance problem which comes up quite often is the problem called N+1. It's when the ORM first fetches the students, and then for each of them performs a separate call to retrieve their enrollments, and then another one to get each of the corresponding courses. So instead of just one database roundtrip, you end up with several of them, and the more students there are, the more roundtrips you will have. In most cases, you can overcome this problem by instructing the ORM to include the navigation properties into the SQL script and fetch them along with the main object, but it's not always possible, and it's very easy to overlook this issue when relying on the ORM to query the database. Alright, and what about the code complexity? Is our code well-structured and easy to understand? Not really. Look at the Update method once again. The fact that it is so long and that there are deep indentations is a sign that something is not right, and indeed, we are trying to do too many things here. Look at the section starting from this if statement. If there is no first course in the incoming student DTO, we disenroll the student from it. Otherwise, we either try to enroll the student in, or modify the enrollment, and the same for the second course. We could probably simplify this code by removing the duplication between the first and the second courses, but still, even with only a single such if statement, there is just too much going on here. This method clearly violates the single responsibility principle, one of the SOLID principles from Bob Martin. Another hint that tells us about the violation is this StudentDto. It is a jack-of-all-trades. It's used for both sending the data to the client, and receiving it back when updating the student. And because of that, some of the fields here remain unused in certain scenarios. For example, when updating the student, the user needs to only specify either a grade or a disenrollment comment, but not both. And there is no need to fill out the credits fields because we don't update them on the server. They are used for displaying information on the UI only. This is a very CRUD approach, and it's not the best way to organize the application code. What we are doing here is we essentially merge all the possible modification to the students into a single giant update method, whereas each of those modifications should have their own representation. This artificial merge of responsibilities entails increase of complexity, which in turn, damages the code base maintainability. This is especially painful in long-running projects where complexity tends to pile up over time and at some point becomes so immense that the whole application moves to the category of legacy applications. It's still valuable for the business, but no one dares to touch it as every modification is likely to introduce new defects. It's important not to allow such a growth of complexity, and Domain-Driven Design in general, and CQRS in particular, are very good at it. We will start untangling our code base in the next module. This thinking in terms of create, read, update, and delete operations is called CRUD-based thinking, and it often goes together with the anemic domain model. Anemic domain model is when you have domain classes that contain data and services classes that contain operations upon this data, but this is not always the case, and it's certainly not the case in our situation. In fact, our domain model is organized quite well and mostly encapsulated. If you look at the student entity, you can see that both collections are hidden from the client code so that it's impossible to modify them. The collections the clients of this class see are both read-only and don't have a setter. The parameter-less constructor is hidden. We expose a nice rich constructor instead that accepts both name and email, which explicitly tells the client code that these two pieces of data are required in order to create a new student. And we also have public methods that check for invariant violations. For example, before enrolling the student in a new course, this method validates the number of existing enrollments. We do have public setters in the name and email properties, but that's only because I have skipped the validation of this data for brevity. And because there's no validation needed, it's okay to keep these setters public. In a real-world application, you would probably want to wrap these properties into value objects. The only issue with regards to encapsulation is in these two methods. Because they are always used together, you always need to add a disenrollment comment when you remove an enrollment. It would make sense to merge these two methods into a single one, but this issue is easy to fix. Other than that, the encapsulation here is pretty solid. Our domain model is not anemic. If you want to learn more about anemic domain models and how to transform them into rich and highly encapsulated ones, check out my course about Refactoring from Anemic Domain Model Towards a Rich One.
Summary

In this module, you saw the initial version of the application we'll be working on throughout this course. The two major drawbacks with it are the use of the single model for both reads and writes, and the CRUD-based thinking. The first issue leads to inability to optimize the database queries. You saw that we had to fetch an excessive amount of data from the database because the ORM doesn't support the kind of querying we need. It's also easy to fall into the N+1 problem where you end up with multiple roundtrips to the database instead of just one. The second issue leads to unnecessary increase of complexity. Because the application tries to fit all operations into the narrow box of create, read, update, and delete operations, what we end up with is the merging of all student modifications into a single giant update method. Another way to look at this problem is view it as the violation of the single responsibility principle; CRP for short. The Update method does too many things at once, which is never a good thing. In the next module, we will start the refactoring away from CRUD-based interface towards task-based interface. You will see how it simplifies the code base and helps improve the user experience.
Refactoring Towards a Task-based Interface
Introduction

Hi, my name is Vladimir Khorikov and this the CQRS in Practice course. In this module, you will learn the difference between the task-based interface and the CRUD-based one. You will also see how we transform the latter into the former.
CRUD-based Interface

One of the most widely spread problems in enterprise-level development is the way programmers approach data modification. As we discussed in the previous module, all operations in an application, fundamentally fall into one of the four categories: create, read, update, and delete; CRUD for short. And it's true. Technically, everything we do is either create, read, update, or delete something, but nevertheless, it's never a good idea to organize your application along these lines, except for the simplest cases. In fact, such an organization can have a devastating effect on your system, and not only in terms of its maintainability. It damages the user experience, too, as you will see in a second. We will call this approach to code design CRUD-based interface, and the overarching mentality, CRUD-based thinking. So, what's the problem with it, exactly? There are three of them. The first one is uncontrolled growth of complexity. As you saw in the previous module, capturing in a single method all operations that somehow mutate an object, leads to enormous expansion of that method. At some point the complexity becomes unbearable. This, in turn, entails lots of bugs when modifying something in the code base and failures to meet project deadlines and quality standards. And this point comes much sooner than you might realize. Even our quite simple application exhibits those traits. Imagine how it would look if we add a few more pieces of data with their own business rules to the student class. The second problem is the disconnect between how the domain experts view the system and how it is implemented. Your domain experts don't speak in CRUD terms, and if they do, it's because you trained them to, not because it's natural for them. Think about it. When you go to college for the first time, does the administrator tell you that she will go ahead and create you in their system? No. She will tell you she is going to register you, not create, and if you decide to take a Calculus course, does she describe it as updating you? Absolutely not! What nonsense. She will enroll you in this course, not update. Another way to describe this problem is as lack of ubiquitous language. Ubiquitous language is one of the three pillars of Domain-Driven Design. It's essential for any enterprise-level development to set up a proper communication channel between the programmers and domain experts, and the best way to do so is to impose a single, unified ubiquitous language within a bounded context, and then have both domain experts and programmers speak this language. And not only speak, but use in it code, too, when defining the APIs of the system. And by the way, if you wonder what the remaining two pillars are, they are bounded contexts and the focus on the core domain. Just as the growth of complexity, this disconnect also leads to maintainability issues. Because there is no consistency between your code and the language of domain experts, you have to translate what those experts tell you every time you speak to them, and this mental overhead is a huge problem. It reduces your capacity to understand the task at hand and leads to inaccuracies in the implementation. On the other hand, elimination of this barrier can make wonders, and so you should always strive to remove as many of such inconsistencies as possible. The third problem is damaging the user experience. You see, the problem of the CRUD-based thinking never stays within the boundaries of your application code. It also affects the UI. The CRUD-based thinking spills over from the code to the user interface, and the same issues that plague the code itself infect the user experience, too. Look at our application for example. If I try to update the student, does it really make sense that when I change the course enrollment to any other course, the interface stays the same, but as soon as I select the empty value, the grade goes away, and I suddenly have to provide the disenrollment comment? Not at all. What's happening here is because there are too many features lumped together on a single screen, the user has to figure them all out on their own. The user is like Sherlock Holmes here. He has to investigate the interface and deduce all the pieces of functionality on a large and complicated scene. Look at this screen once again. How many features are in here? At least four. Editing the student's personal information, enrolling them into a new course, transferring to another course, and disenrolling them from a course, and it will take a while for a user to uncover them all. Because of that, users have a hard time building a proper mental model of the product and grasping even the most common procedures. In complicated systems, many users never master all of those procedures, even after working with the software for a long time. All these features have to be separated out. The UI should guide the user through the process, not crash them with the full application functionality on a single giant window. Overall, the focus on user experience should become an integral part of the development process, which almost never happens when the CRUD-based thinking is in place. As you can see, CRUD-based thinking affects both the code base and the user experience. This is why I call the end result CRUD-based interface; not user interface, but just interface. It's because this thinking affects both the API, application programming interface, and the UI, user interface. The two influence each other. If you have this problem with one, you will almost certainly have the same problem with the other. It's also interesting to take a short look at where this thinking originates from. Indeed, why is CRUD-based interface so widely spread? It's because of the programmers' desire to unify everything. Just look at the premise of object-oriented programming, which states that everything is an object, and so why not take the same approach and define every operation in terms of CRUD, too? From the perspective of us as programmers, this leads to a nice-looking and clean system, where all of the APIs have one narrowly defined purpose. Who wouldn't like it? But of course, this doesn't lead anywhere. What you need to do instead is talk to domain experts, discover proper terms, and adjust your thinking accordingly.
Task-based Interface

So, how to fix this issue? The opposite of CRUD-based interface is task-based interface, and that's what we need to replace our current design with. Task-based interface is the result of identifying each task the user can accomplish with an object in the application. This is why it's name is task-based, and assigning a separate window to each of them, and by extension, introducing a separate API endpoint, too. This idea takes root in the concept of intuitive UI. Each window should implement a single distinctive operation. We need to restore the single responsibility principle, so to speak, and untangle our over-complicated update window and the giant update method. Not only will this lead to the code base simplification, but it will also bring a great amount of domain insight, just by virtue of implementing this separation, because you will have to research what each feature on the screen does and means to the domain experts. Because of all of that, you will be able to deepen your knowledge of the problem domain and bring your code base closer to the ubiquitous language. And the users will benefit greatly, too. The task-based interface makes it much easier for them to explore the software and learn what he or she can do with it. Currently, the application's business process is in the minds of people who developed it. Users must discover the correct process on their own. After moving towards the single responsibility principle and the task-based interface, each window on the screen becomes self-describing and intuitive.
Untangling the Update Method

Alright, here's our update window again. Currently, it is object-centric, meaning that it tries to deal with the whole student object. What we need to do instead is split it into several task-centric windows, each accomplishing its own separate task. As we discussed before, there are four of such tasks. Editing the student's personal information, enrolling into a new course, transferring to another course, and disenrolling from a course. So, let's do that. Here's our Update method. Note that these comments here are trying to translate for the reader what is going on in this method. For example, if there's no incoming course name, it means that the student disenrolls. If no enrollment currently exists, that means the student enrolls, and so on. This is already a sign that the method does too many things. We will untangle the method by extracting all these pieces of functionality into their own API endpoints. We'll start off with the enrollment. I'm naming the new method, Enroll. Just like the update one, it will accept the student id and some DTO. This DTO, however, should be different. It should contain only the information about the enrollment, nothing more. So, I'm renaming this copy to StudentEnrollmentDto. Getting rid of this, the class will consist of a course name and a grade. We don't need anything else for the enrollment process, so I'm removing the remaining properties. Alright, the first step would be to fetch the student from the database and return an error if such student doesn't exist. Here is the old code once again. In order to enroll the student, we need to retrieve the course from the database and parse the grade. So let's do that. Get the course by its name. If such course doesn't exist, return an error saying that the course is incorrect. Next, parse the grade. If unsuccessful, also return an error and tell the client that the grade they passed in is invalid. Finally, we can copy this part of the method to our new API endpoint. Replace the second parameter with the grade. Good, and commit the transaction and return a success. Very good. We can remove this part from here. The next method is going to be student transfer. So let's create another API endpoint, and by the way, we need to assign a URL and an HTTP method to the student enrollment. Let it be this. Okay, I'm calling the second endpoint Transfer. It will accept the student id as the first parameter, but aside from that, we also need to specify an enrollment number. This number would tell us which of the enrollments the student wants to modify. We didn't need this number before because the Update method accepted information about all enrollments in a single DTO class, but because we are untangling that class, the UI will have to indicate which of the enrollments is changing. Alright, and of course, we will also need a DTO. I'm copying the StudentEnrollmentDto, because it has the same properties we need, and renaming it to StudentTransferDto. Note that these two classes are exactly the same, and so in theory, we could use the same DTO for both endpoints. However, this is generally a bad practice, because it introduces coupling between the two endpoints. Now if the parameters required for one operation change for some reason, and we modify this DTO, the second API endpoint will automatically change its input parameters, too. That is something you want to avoid. Also, there is no violation of the don't-repeat-yourself principle here. These two classes just happen to coincide with regards to their structure. There is no domain knowledge duplication. So, the Transfer method will accept this new DTO. We will need the same validations as in the Enroll method so I am copying them here. Now, we need to get the enrollment itself from the student somehow, and currently, we only have these first and second enrollment properties. We need to adjust the student class a little bit. As you can see, the code is already there. The two helper properties use the method that accepts the enrollment number. So we just need to make this method public, and because we are using NHibernate, we also need to make this method virtual. Now we can use this method in the controller. Passing the number, and introducing a variable. If the number is incorrect, we are returning an error. After all the preparations are done, we can finally copy this line of code from here. Fixing the parameter, and the enrollment, and also committing the transaction and returning an OK. We can remove this piece of code from here, and this whole section and this one, too. The next one is student disenrollment, but first, let me put a binding before the transfer method. Good. So I'm calling the third method Disenroll. Here is the binding for it. It will also accept the student id and the enrollment number. As for the DTO, it will be StudentDisenrollmentDto, and it will have a single parameter, this comment from here, using it here. Alright. The first validation is also about the student. Aside from that, we need to ensure that the comment is not null or empty. So I'm copying this validation from here. Getting the enrollment by the enrollment number, and the action. Removing the enrollment, and adding a disenrollment comment. Changing the property, and return OK. Note that here, we are using two methods from the domain model to accomplish a single task. This is a code smell. It is a sign that the domain model is not properly encapsulated and is leaking internal implementation details. And indeed, neither of the two operations make sense without the other one from our domain model's perspective. We cannot remove an enrollment without adding a disenrollment comment, and we cannot add a disenrollment comment without removing an enrollment. So let's fix this. I'm moving the code from the add disenrollment comment method here, copying the comment parameter, and removing this second method altogether. Fixing the parameters, and removing the invocation, too. Very good. Now it's impossible to do one action without the other. Let me remove these lines, and actually, we don't need this whole if statement anymore. We extracted all of its functionality into separate API methods, and the same is true for the second if statement, which deals with the second enrollment. Alright, the last piece of functionality is editing the student's personal information. Introducing a new method for it. The binding is going be the same as for the update one. As for the DTO, we only need a name and an email. So I'm adding a new DTO containing these two properties. Copying this code from here, and committing the transaction. Now we can get rid of the whole update API endpoint. We have extracted all the functionality out of it, and this private method. Perfect.
Recap: Untangling the Update Method

In the previous demo, you saw how we untangled the student update method. We have split it into four methods, each implementing its own distinctive task. Editing the student's personal information, enrolling into a new course, transferring to another course, and disenrolling from a course. In other words, we refactored the update API endpoint, which had a CRUD-based interface into several smaller ones, which are now task-based. Each of those methods adheres to the single responsibility principle, meaning that they are responsible for doing one task and one task only. That, in turn, allowed us to greatly simplify those methods. Look at the disenroll one for example, and now compare it to only a part of the update method. The cyclomatic complexity of this one, which you can measure by counting the number of indentations in code, is higher here, and it's longer, too. It's much harder to understand what is going on here, whereas the new method is simple and straightforward. The cyclomatic complexity here is low. The singular indentations are related to input validation, and you can see a clear pattern in all of the four methods. First, we do the validation, which takes some space, but is simple to follow nevertheless. Then, we delegate the actual work to the domain model, and after all that, commit the transaction and return the response to the client. Note that along with the API endpoint, we modified the DTOs we are using, too. Before the refactoring, the Update method relied on this large and clunky StudentDto in which we lumped together fields to accommodate all possible modifications that could be done to the student. And not only that, we used this same DTO for displaying students' info in the grid. Hence even more fields that are unnecessary when updating the student. The new task-based interface now has a separate DTO for each of the tasks. All of them are small and concise and have only the info required to accomplish the task at hand. We have removed all the redundancies from the DTO, and instead of one coarse-grained, one-size-fits-all class, have several fine-grained ones. The takeaway here is this. Avoid DTOs that are full of holes, and holes here mean fields that are not used in 100% cases. They are a strong sign you have a CRUD-based interface. This simplicity is an inherent part of the task-based interface. Because we don't try to fit all modifications into a single giant update method anymore, we are able to come up with a much more precise and straightforward solution. Each task provides us with the right level of granularity and intent when modifying data. Alright, so we were able to simplify our code after refactoring towards the task-based API. Let's now see how this change affects the user interface.
Task-based User Interface

I won't bother you showing how I'm refactoring the UI. After all, this course is not about user interfaces, but it's still interesting to look at the end result, so here it is. First of all, you can see that I have renamed this button to Edit Personal Info. It was Update Student previously. And if I click it, it shows me a much simpler window with just the personal information of the student. The rest of the functionality is now represented as context-dependent buttons in the grid itself. For example, Alice is not enrolled in her second course, and so here you can see this button, which I can use to enroll Alice in a new course. And this window is very small, too. All fields here are dedicated to accomplishing this specific task, enrolling the student in some course, which makes the interface simple and intuitive. Let me refresh the grid. The two remaining actions show up only when they are applicable in the current context; that is, when the student is already enrolled in some course. You can see that I can transfer Alice from Literature to, say, Microeconomics, or I can disenroll her from this course, in which case I need to specify a disenrollment comment. Note that this comment is the only piece of information I need to provide when disenrolling the student, which makes a lot of sense if you think about it. Everything else can be deduced from the context. No need to explicitly specify the course she is disenrolling from. So, when I type in some comment, and refresh the grid, you can see the second course is gone now, and the application offers me to enroll Alice again. This natural flow of events makes for a great user experience. Every single operation is intuitive and asks for the minimum amount of information to accomplish the task at hand. Now compare it to the old version with the CRUD-based UI. All four operations are lumped together on a single big screen. The controls appear and disappear depending on the user input, and it's not obvious at all that if I select an empty string for the first course, it means that I'm disenrolling the student.
Dealing with Create and Delete Methods

Alright, we've dealt with the Update method. We extracted it into four smaller, task-based ones, but what about create and delete API endpoints? At first glance, they seem fine. The Delete method is small and on point. It might look like the create method does too many things. After all, it both creates a student and enrolls them into courses, but actually, no. Having these two operations together helps user experience. Users normally perform these two actions, one after the other, and so having them on a single screen simplifies this task. There still are two problems with these methods, though. First, the Create endpoint still uses StudentDto to accept data from the outside. It is the same DTO the UI uses to display the students in the grid, and it has fields we don't need when creating a student, for example, id and course credits. So, we need to come up with a separate DTO that would contain only the fields required for this task. The second problem is the names of these methods themselves. Think about it. Does it make sense, from the domain's perspective, to create or delete a student? How do the domain experts describe these operations? Remember, any API endpoint that is named after one of the CRUD operations is a red flag, and each time you see such an endpoint, ask yourself, is this operation really just that, create or delete, or maybe there is a deeper meaning behind it? This habit alone can give you a great amount of insight into the problem domain you are working on. So, what do these methods actually mean? The only way to find out is to ask the domain experts. As we discussed earlier, they probably say something like register when they create a student, and unregister when they delete one. And so it makes sense to incorporate this piece of ubiquitous language into our code as well. Alright, let's do that. Copying the DTO. Renaming it to NewStudentDto, and removing all fields we don't need. Id, disenrollment comments and course credits. Now we can use this DTO here. Renaming the Create method to register, and the Delete one to unregister. Very good. Note that we now use the StudentDto class only for displaying students in the data grid, and so, we can get rid of the fields that were used for modifying the students. In particular, we don't need the disenrollment comments anymore. And here's how the UI looks now. I modified it off-screen. The Register Student screen still contains all those fields, but no disenrollment comments anymore. Let me register a student, James Bond; enroll him in Calculus, and I don't think we was very good at Math. Very good, and I can unregister this student just as before. Perfect. Our interface, both API and UI, is now completely task-based. Note, however, that task-based interface is not a prerequisite for CQRS per se. You can have one with or without the other, but the problem of CRUD-based thinking often goes hand in hand with CQRS. People who suffer from having a single model that handles both reads and writes usually also suffer from CRUD-based thinking. Also note that sometimes, CRUD-based interface is just fine. It's the same situation as with anemic domain models, really. If your application is not too complex, or you are not going to maintain or evolve it in the future, no need to invest in task-based interface. Basing your interface on CRUD operations can give a short-term boost, which might be helpful if the development of the application is not going to last long enough to justify the investment.
Summary

In this module, you saw how we refactored our application towards task-based interface. You learned what CRUD-based interface and CRUD-based thinking is. CRUD-based thinking is when people try to fit all operations with an object into a narrow box of create, read, update, and delete operations. CRUD-based interface is the result of CRUD-based thinking. Note that the word interface includes both user interface, UI, and application programming interface, API. It's because the two influence each other. If you have this problem with one, you will almost certainly have the same problem with the other. There are three problems with CRUD-based interface. The first one is uncontrolled growth of complexity. You saw that capturing in a single method all operations that somehow mutate an object leads to this method being overcomplicated and hard to maintain. The second problem is the disconnect between how the domain experts view the system and how it is implemented. Your domain experts don't speak in CRUD terms. They don't say create or update a student, and just this act of listening to domain experts and trying to incorporate the business language into the application leads to great domain insights. The third problem is damaging the user experience. CRUD-based interface leads to too many features being lumped together on a single screen so that the user has to figure them all out on their own. The resulting UI is not intuitive. It is object-centric, meaning that it tries to deal with the whole object at once. Task-based interface is the opposite of CRUD-based interface. It is the result of identifying each task the user can accomplish with an object and assigning a separate window to each of them. This also affects both the UI and the API. In terms of the UI, the single object-centric window gets split into several task-centric ones. In terms of the API, you introduce several API endpoints dedicated to accomplishing one single task. As a result, both the UI and API becomes much simpler to understand and maintain. For example, you saw how we untangled the single update method into several much smaller ones and the UI started to make much more sense, too. Try to avoid CRUD-based language when it comes to naming your endpoints. It's a sign of CRUD-based thinking. For example, in our sample application, we renamed Create and Delete methods into Register and Unregister, because that's what they meant to the domain experts. Another sign of CRUD-based interface is when you have DTOs in which some of the fields are not used in 100% of cases. For example, the CRUD-based version of our application used the StudentDto class with the fields DisenrollmentComment and Grade. Those fields got populated in particular scenarios only and were left empty in all other cases. That's because our API was CRUD-based and tried to do too many things at once. In the next module, we will segregate commands from queries in our application, the essential part of the CQRS pattern.
Segregating Commands and Queries
Introduction

Hi. My name is Vladimir Khorikov and this is the CQRS in Practice course. In the previous module, we refactored our application towards the task-based interface. All the operations the student can perform with it are now clearly defined and have their own API endpoints. Our next goal on the path to CQRS would be to introduce explicit commands and queries for each of those API endpoints. This will help us down the road by bringing in the benefits of CQRS; the three benefits we discussed in Module 1: scalability, performance, and simplicity.
Introducing a First Command

Alright, so here it is, our StudentController. Let's first outline which of the API endpoints here represent commands and which queries. As we discussed in the first module, any operation can be either a command or a query. A query doesn't mutate the external state, such as that of the database, but returns something to the caller. A command is the opposite of that. It does mutate the external state, but doesn't return anything to the client. So, which one is the GetList method? The GetList method is clearly a query. It returns a list of students that are enrolled in some course, if it's specified by the client, and that are enrolled in a particular number of courses. The method doesn't mutate any data. As you can see, all it does is it selects the students from the database and transforms them into DTOs, which are our data contracts. The Register endpoint, on the other hand, is a command. It does mutate the database state. It inserts a new student into it, and it doesn't return anything to the caller other than the acknowledgment of the operation success. So, I'm marking this method as a command. Unregister deletes the student from the database. So it's a command. The same is true for the Enroll method, Transfer, let me put this to a separate line. Transfer is also a command. Disenroll and EditPersonalInformation. Both of those are commands, too. As you can see, the separation between the endpoints in terms of which falls to which category is pretty clear, and it aligns perfectly with the REST best practices. Put and Post http methods correspond to commands, while endpoints like GetList, which are marked with the GET http method, they are queries, and this alignment is not a coincidence. REST fundamentally follows the Command-Query Separation principle. If you execute a Get query, it should not modify the state of the REST resource, which is the same as executing a query in the CQS sense of that word, and if you run a POST, PUT, or DELETE query, that does change the state of the resource, and that would be a command in the CQS taxonomy. To continue down the CQRS path, we need to introduce separate classes for each command and query in our application, and so we will do exactly that. We'll start with the EditPersonalInfo method. Let's create a new class. I'm calling it EditPersonalInfo command. Make it public and sealed. Good. So the command will contain the data from the DTO, the name and email properties. And I also need to add the student id of type long. Now, what we could do with this command is we could first instantiate it in the controller method, and then somehow execute, for example, by calling an Execute method on the command itself. That would be the first choice for most of us, programmers, who want to make sure that the command is properly encapsulated. However, this is not the best design decision, because we would conflate two different concerns here. Command itself should represent what needs to be done. It should be a declaration of intent, so to speak. The execution is a different matter. The execution often refers to the outside world, such as the database and third-party systems, and you don't want to delegate this responsibility directly to the commands. This is an important point, and we will talk about it in more detail later in this module. So, because we don't want the command to execute itself, there should be a separate class that does that, and here it is, EidtPersonalInfoCommand handler. It will contain a single method, Handle, that accepts the command. What we can do now is we can instantiate that handler in the controller, and pass the command to it. Good. And of course, we need to instantiate the command itself properly. Setting the Email, Name, and Id, and let me comment all this, and move it to the handler itself. Now, the problem with this design is that we need to instantiate the handlers manually in each controller method, which would be quite repetitive, or we would have to inject them into the controller's constructor, like this. As you can see, there are quite a few of controller methods here, and we would need a handler for each of them, and so you can imagine that the number of parameters in the constructor will get out of control very quickly. You don't want to find yourself in this situation either, because that would damage the maintainability of the code base. What we can do instead is we can leverage the ASP. NET Core dependency injection infrastructure, and we will do that shortly. For now, let's remove this handler from here, and get back to the EditPersonalInfo method. Another problem with this approach is that each handler class will have their own public API. There would be no common interface between them, and so we won't be able to extend them with new functionality. That's something we are going to do moving forward, introduce decorators on top of the command handlers so that we could enrich those handlers and introduce cross-cutting concerns to our application. And so we need a common interface between all our commands and command handlers. For that, we need to introduce a couple new types. The first one is ICommand. It's a marker interface, which means that its sole purpose is to mark the commands in our code base, like this. The second one is ICommandHandler. It's the interface that works with a particular type of commands, and this type is specified here with the TCommand type parameter. We can put a restriction here that would require TCommand to be of a type that implements the ICommand interface. The interface will have just a single method, Handle, that would accept a command of type TCommand. Let me put the ICommandHandler interface closer to ICommand. Now we can inherit our command handler from this interface and put the EditPersonalInfo command as the type parameter here. This way, all our command handlers will have a common interface, which would make it easy to decorate them later. Alright, so having this groundwork laid out, let's uncomment the code in the handler and fix the compilation errors. As you can see here, the handler needs a student repository. In the controller, we create the repository in the constructor, by instantiating it manually. For that, we use an instance of UnitOfWork that ASP. NET Core injects into the constructor, and just to remind you, UnitOfWork is our own wrapper on top of NHibernate session and transaction. So, we can apply the same approach in the handler. I'm creating a constructor that would accept a UnitOfWork, and save it to the local field, making this field ReadOnly. Good. Next, instantiating the student repository in the handle method. Use it here. Put command. Id instead of just id, the same here, and get the name and the email from command as well. Now, all the compilation errors are fixed, except for this one. So, what are we going do about it? If we look in the controller, and open the definition of this method, you can see that it is defined in our base controller class. It's basically a wrapper on top of the built-in BadRequest action result, and we pass an envelope with the error to it. We could make our command handler return an instance of IActionResult, too, but that's not the best design decision. It's better to leave the ASP. NET concerns to the controller, and keep the command handler free of such concerns, but how can we return an error from the command handler then? That's a good question. One way could be to throw an exception, which is also not the best way to deal with this issue. Exceptions used for controlling the program flow and specifically for validation, tend to complicate the code base, and it's better to use an explicit return value instead. You can learn more about exceptions and the common misconceptions about them in my Applying Functional Principles in C# course. And so we will use a special Result class. It's a simple class that represents either a success or a failure of an operation. It's located in a small NuGet package I created called CSharpFunctionalExtensions. There is no need to use this specific library. You can easily come up with an implementation of your own. Again, if you want to learn more about the reasoning behind introducing such a class, refer to my Applying Functional Principles in C# course. Now that the Handle method returns a result instance, here, we can call Result. Fail instead of error, and we also need to update the signature of the interface. Change the return type from void to Result. Very good. If we go back to the controller, we can instantiate the handler, pass the UnitOfWork instance to the constructor, get the result back from the Handle method, and then parse it. If the result is successful, call OK, and if not, call Error with the error from the result instance. Perfect, and of course, we also need to return Result. Ok in the handler. That would mean that everything worked fine. Let's run the application and make sure we didn't break anything. Edit the personal info of Alice, change her name to Alice 2, and you can see that the name has indeed changed correctly; very good.
Commands in CQS vs. Commands in CQRS

In the previous demo, we introduced our first command, EditPersonalInfo command, and a command handler for it. We moved all the code from the corresponding controller method to that handler. Now, all the method does is it instantiates the command and the handler for it, passes the former to the latter, gets a result out of the handler, and then interprets that result by returning either a 200 or a 400 response; 200 being a success, and 400 being a validation error here. Before we move forward, let's clarify some potentially confusing terminology. You might have noticed that we've been using the term Command in two different ways. First, the controller method, and second, the command class. The difference between them is that the former is a command in the CQS sense. Remember, we discussed in the first module that the command-query separation principle states that all methods in a class should be either commands or queries. This method is a command because it modifies the application's state and doesn't return any data to the client aside from the confirmation that the operation is completed successfully. The second one, the class, is a command in the CQRS sense. It's an explicit representation of what needs to be done in the application. It's a simple class named after the operation; in our case, the EditPersonalInformation operation, and it contains the data required to perform that operation: the id, the name, and the email of the student. You can think of commands as being serializable method calls. So, there should be no confusion, really. It's just the term Command is overloaded and has many meanings depending on the context you operate in, and the same is true for queries. The term query is also overloaded and means a query-method in the context of CQS, and a class in the context of CQRS. Alright, so a command is a serializable method call, so to speak. Is there a similar analogy for a command handler? It turns out, there is. You can think of command handlers as being regular ASP. NET controllers with just a single method. Here you can see our command handler again. The most fascinating part here is that that single method is also a command in the CQS sense, because it mutates state and doesn't return anything aside from the operation confirmation. It's interesting to see how these concepts are interconnected with each other.
Commands and Queries in CQRS

Now that we have our first command, let's discuss commands and queries in more detail. All messages in an application can be divided into three categories: commands, queries, and events. A command is a message that tells our application to do something, a query is a message that asks our application about something, and an event is an informational message. Our application can generate an event to inform external applications about some change. Here's how all three can be depicted together on a single diagram. As you can see, the client sends commands and queries to our application in order to either tell it to do something or ask about something. On the other end, our application communicates with external applications via events. It informs them about changes within the app. We will not be focusing on events much in this course. Just keep in mind that it's the same concept as domain events. You can learn more about domain events in my other course, Domain-Driven Design in Practice. There are naming guidelines associated with all these three types of messages. First of all, commands should always be in the imperative tense. That's because they are telling the application to do something. EditPersonalInfoCommand is a good example here. It tells our application to edit, to modify the personal information of the student. Queries usually start with the word Get, for example, GetListQuery. That's because queries ask the application to provide some data, and it's hard to come up with something else other than the word Get for such a request. Finally, events should always be in the past tense. That's because they state a fact that already happened; some event that is significant for your domain model. For example, let's say that our application needs to convey to the marketing department the updates to the students' email addresses, so that the marketing department has correct contact information to send promotional materials to. To do that, we could raise a domain event, something like PersonalInfoChangedEvent, and the marketing department could subscribe to that event and update their records accordingly. Note the difference in the semantics here, EditPersonalInfo versus PersonalInfoChanged. This distinction is very important. By naming the command Edit Personal Information, you imply that the server can reject this message. It can reject executing it, for example because the email in that command is invalid. On the other hand, the application cannot reject an event. If your application receives an event named PersonalInfoChanged, there is nothing it can do about it. The operation it informs you about has already happened, and this event is just a notification about that fact. The only thing the application can do is update its records, and so naming in either imperative or past tense makes it absolutely clear which message that is. It makes it clear whether the message is a command or an event. Another important guideline with regards to the command names is that they should use the ubiquitous language. If you see a command named CreateStudent or UpdateStudent, it's a sign of the CRUD-based thinking, and, as we discussed in the previous module, it leads to the CRUD-based interface, which is hard to maintain and work with. Make sure your commands are task-oriented, not CRUD-oriented. We already solved this problem in the last module. All our API endpoints are task-oriented, so I won't dwell on it too much here. And one last thing about naming. You saw that I used postfixes when naming commands, queries, and events; the Command, Query, and Event postfixes. However, it's perfectly fine to leave them out. If you follow the above guidelines and always name the commands in the imperative tense, events in the past tense, and start queries with Get, you shouldn't have any problems with distinguishing these three types of messages, and so there's no need really in keeping the postfixes. It's your call, though. If you are more comfortable with keeping them, it's fine, too.
Commands and Queries in the Onion Architecture

You might remember this diagram from my Domain-Driven Design in Practice course. It's the onion architecture that shows elements of a typical application. The inner layer represents the core of the domain model. It's the entities, aggregates, value objects, domain events, and pure domain services with which you model the domain. This is the most important part of your system, the one that you need to pay attention to the most. One level beyond that, there are parts of the domain model that touch the external world; the database, third-party systems, the file system, and so on. Finally, there are other application services and the UI, which don't belong to the domain model. Such layering helps show the relationship between different parts of the application. The fact that the core domain resides in the center of the onion means that classes from it cannot access classes from the outer layers. The core domain should be isolated from the external world. It should be self-sufficient. Alright, so where do commands and queries belong in this picture? Think about it for a moment. I'll wait a couple seconds. Ready? Okay. You actually had a hint here. Events are already presented on this diagram, and they are located at the core layer of the onion, and that's right; commands and queries also belong to that layer, just like domain events. All messages in your application are part of the core domain model. They explicitly represent what the clients can do with the application; that would be the commands. What questions they can ask that would be the queries, and what the outcome is for the external applications. That's the events. You might be hesitant to treat commands and queries as part of the core domain, and if you are, consider this. Commands and events really reside at the same abstraction level of the workflow. Commands are what trigger a reaction in the domain model, and events are the result of that reaction. They are the two ends of the same stick. The only difference between them is that the commands follow the push model, while the events, the pull one. In other words, it's someone else, not our application, that raises the commands. That would be the push model, and, on the other hand, it's us, our application, that raises the events. That is the so-called pull model. And when you start looking at commands and queries as parts of the core domain, it becomes clear why you shouldn't implement the handling logic in those commands and queries themselves. It's all about domain model isolation. The classes inside the core domain shouldn't refer to the outside world, because that would violate the domain model isolation. Those classes should only know about classes in the same layer. They should know nothing about the external world or the classes in the outer layers of the onion. At the same time, when you handle a command, you do need to refer to the external world, or to the outer layers of the onion. In our case for example, we edited the personal info of the student, and to do that, we worked with the student entity itself, and we also worked with the repository, and that repository talked to the database, a component that is external to the core domain. Alright, but where to attribute the handlers then, you might ask? That's a good question. They implement the same role as the Application Services do, so command handlers should go here, to the Application Services layer, and if you remember, before we introduced the EditPersonalInfo command handler, the code that was responsible for handling this scenario resided in the controller, and this controller is part of our Application Services layer. So when we moved this code to the handler, we just relocated it from one part of the Application Services layer to another.
Commands vs. DTOs

Let's now talk about the difference between commands and DTOs, data transfer objects. Look at our EditPersonalInfo controller method once again. Here, we are accepting a DTO from the client, and then transform it into a command. The reason why people don't often attribute commands to the core domain is that they often replace the DTO with the command itself. They skip this mapping stage, and transform the controller into something like this, and so in this case, because the commands come from the outside of our application, it would indeed be problematic to treat them as part of the core domain model. The solution to this problem is that you shouldn't do that. Commands and DTOs are different things. They tackle different problems. Remember, commands are serializable method calls, calls on the methods in the domain model, whereas DTOs are the data contracts. The main reason we introduce this separate layer with data contracts is to provide backward compatibility for the clients of our API. Without the DTOs, our API will introduce breaking changes with every change of the domain model, or we will not be able to properly evolve the domain model because of the constraints imposed by the backward compatibility. Either way, using the commands in place of DTOs forces us to choose one of the two suboptimal design decisions. On the other hand, the DTOs and the mapping between them and the commands ensures that our application will be both backward compatible and easy to refactor. Using commands in place of DTOs is similar to using domain entities, which is extremely harmful for the encapsulation of your app. It's not as bad, of course, because commands themselves don't contain any business logic, but the drawbacks are very similar. Both of these approaches hinder your ability to refactor the domain model. For example, if we decide to split the student name into first and last names, we won't be able to modify the command itself in order to keep the backward compatibility. We will have to keep the existing, obsolete version of the command, and implement the mapping in the handlers instead. It's much cleaner and more maintainable to just have two layers, the DTOs and the commands, each playing their own part. DTOs for backward compatibility where you can have as many versions of the data contracts as you want, and commands for the explicit representation of what the clients can do with the application. It's much easier to implement the mapping between the two than to try to lump all these responsibilities into the same class. I talked about the issue with the backward compatibility in my course about anemic domain models, so check it out for more details. Note that if you don't need backward compatibility in your application, then it's fine to use commands in place of DTOs. For example, if you have a single client, which you also develop yourself, and you deploy both of them at the same time, then there is no such issue as breaking the clients of your API. Because both of them will have the latest version deployed simultaneously, there would be no such situation where you have an old version of the client talking to the new version of the API, but keep in mind that it's an edge case. Treat it as a code complexity optimization technique. In a case of publicly available API, or if you cannot deploy the client and the API simultaneously, you do need both, the DTOs and the commands.
Leveraging ASP.NET Core Dependency Injection to Resolve Handlers

I mentioned previously that it's not very convenient to create handlers in each and every controller action manually. Here you can see we do exactly that. We manually instantiate the handler, and then pass the command to it, and here is the handler itself again. What we can do instead is we can leverage the ASP. NET Core dependency injection mechanism to resolve the command handlers for us. For that, let's create a helper class. I'll name it Messages, make it public and sealed; good. This class will be responsible for dispatching all our messages, all our commands and queries. Create a constructor. The constructor will accept an instance of the IServiceProvider interface, and save it to a private field. This interface is from ASP. NET Core; it's part of the ASP. NET component model. It implements the service locator pattern, gets from the dependency injection container a service of a given type. These are the types that you define in the startup class. Here it is. So, when you do this, add a session factory as a singleton, add a UnitOfWork as a scoped service, you tell ASP. NET how to resolve dependencies needed for instantiating controllers and middleware, but you can also ask ASP. NET to provide you with those services explicitly, if you call the GetService method on the IServiceProvider interface. So here, I'm adding the Messages class as a singleton, because there should be only one such class in our application. And I'm also adding our command handler. This line will tell ASP. NET how to resolve the ICommandHandler interface of the type parameter, EditPersonalInfo command, and we also need to change the lifetime of the UnitOfWork class from scoped to transient, because our version of this class doesn't have a dispose method. Okay, let's go back to the Messages class. It will dispatch commands using this, public void Dispatch method. Here, we need to find a handler for the given command instance. o, how to do that? We have the provider, which already knows how to resolve a handler for a particular ICommandHandler interface. We told it how to do that in the application Startup. Now, we need to compose that interface and feed it into the provider, and we'll do exactly that. First, get the type of the ICommandHandler interface. Here it is once again. Along with the interface itself, we also need to get the type parameter. We can do that by calling the GetType method on the command instance. Next, we create a generic type of the given interface and the given type parameter. So this handler type here contains this value now, and because the provider knows how to resolve this type, we can ask it to do that. Here's our handler of type object. Let me make the provider ReadOnly. Good. This handler is of this type, EditPersonalInfo command handler. Right now, because there is only one handler in our code base, we could just cast the handler object to this type and call the handle method on it, but that obviously doesn't scale, because with every new handler, we would need to update the dispatch method in messages. So we need a universal approach to calling this handler. handle method. We could employ reflection further, but there's a simpler way. We can just cast the handler to dynamic type and let. NET dynamic binding mechanism do its work. So, call Handle, and pass the command to it, which should also be casted to dynamic first. Get the result, and return this result. Note that the handle method in the ICommandHandler interface returns an instance of Result, and so we can use it here, and also change the return type of the dispatch method to result as well. So now we can go to the controller's constructor, and add a new dependency, an instance of the Messages class. Save it to the local field, and make this field ReadOnly. In the controller action, we can now call messages. dispatch. That will return a result, and we can remove these two lines. Very good. Let's verify that the application still works. Let's edit the personal info of Alice to Alice 2, and you can see the name is indeed updated. Everything works as before.
Introducing a Query

As you can see, it's quite useful to leverage the built-in ASP. NET dependency injection container. It does exactly what we need with very little effort on our part. Let's now introduce a query. In our application, there is currently only one method that is a query in the CQS sense, and that is GetList. As you can see here, it returns a list of student DTOs. So, let's go to our command. Here it is. Just like the ICommand interface, we need to create an IQuery one, and another interface for query handlers, where TQuery should implement the IQuery interface. It will also have a single method handle. Instead of TCommand, the type of the input parameter will be TQuery, and I'm renaming the parameter itself as well. Note that the command handler returns just a result instance, and that's fine, because commands generally don't return anything other than the confirmation of the operation success, or failure for that matter, but the query handler will have to provide some data, and that data will vary depending on the particular query. And so there can be no one-size-fits-all solution here. Alright, how to fix this? By introducing a new type parameter, of course. So, along with the type of the query itself, we also need a type of the result the query returns, which the handler then will return to the caller, and to properly dispatch the query in the Messages class, we will also need to introduce this type parameter to the IQuery interface itself, as well. You will see shortly why we need this. Alright, after all this preparation is done, we can finally proceed with the query itself. The query is going to be GetListQuery. It will implement the IQuery interface of type list of student DTOs. You can see the compilation error here. Our command handler doesn't know about the StudentDto. That's because this DTO is defined in the API project. Ideally, all DTOs, all data contracts, should reside in their own assembly, and that would fix this error. Both API and Logic projects would then reference this separate project with data contracts, but here, I will just move all DTOs to Logic for brevity. Let me delete this folder from here, and change the namespace here. Good. Now we can reference the DTO from this class, and while we are here, let me also move Messages to the Utils folder. Perfect. As you can see in the controller, the GetList method has two parameters. So we need to add them to the query; the course the student is enrolled in and the number of all enrollments, generating a constructor, good, and let's also refactor our command as well to make it immutable. Removing setters and generating a constructor. Now, the query handler. It should implement the IQueryHandler interface, the type of the query is GetListQuery, and the type of the result is list of students. Generating the handle method, just like in the command handler, here, we also need a reference to the UnitOfWork class. I'm copying the two lines from the controller, and the convert method. With the UnitOfWork, I can now create a repository, use it here. We can take the parameters from the query parameter. Enrolled in, number of courses, and return the DTOs. Good. Let's simplify the method a little bit. We can inline the repository instance, like this, inline the students instance, and also inline the DTOs. Perfect, the handler is ready to be used. In order to use it in the controller, we need to set it up in the startup class. Here, we specify that the GetList query handler implements this IQueryHandler interface, and we also need to add another dispatch method to the Messages class. This one will accept IQuery of T, where T is the type of the return value, and it will return that type back to the caller. That's why we needed to parameterize the IQuery interface, because otherwise, there would be no way for us to parameterize the return type of this method. Defining the type of the handler, note that this handler has two type arguments, one for the type of the query and the other one for the type of the result, and so we need to define both of them here. Creating the type of the handler, getting the service from the provider using that type, and doing the same trick with dynamic typing as in the command dispatcher, but this time, casting the return value to result type. Finally, returning this result to the caller. Now, in the controller, I can write messages. dispatch, create a new query using the two parameters, and returning the list, and I can remove these two lines from here. We have one compilation error left. Instead of instantiating the command using its property setters, we need to pass all its parameters to the constructor, like this. Good. Let's make sure that the application works after this refactoring, and here you go. All the students are successfully displayed on the screen. We can filter them by the number of courses, or by the course they are enrolled in, and it all works. Perfect.
Finishing up the Refactoring

We've implemented one command and one query so far. Let's finish up the refactoring and move all the remaining code from the controller to commands. We will start with this Register method. It will need a command. As for the properties, they need to be the same as in the DTO here. So, here they are, and a constructor. Creating a handler for it. It should implement ICommandHandler from RegisterCommand. Just as other commands and queries, this handler needs access to the database, so I'm injecting a UnitOfWork into its constructor, the handle method that accepts the register command. I can cut and paste the implementation from the controller action. It needs two repositories, course and student repositories, to do its work, and let's fix the compilation errors. Replace all the DTO occurrences with command, courseRepository, studentRepository, other commands. Import the system. enum type, these two, and finally, instead of calling Ok, I need to call Result. Ok. In the controller, I'm instantiating a command using the data from the DTO, dispatch this method, and transform the result into an http response. The next one is unregister. Defining a command. It only contains a single field, id, because that's what the controller method accepts, and here is the handler for it. Dispatching this command in the controller action, and return a result. I'll speed the rest of it up. It's mostly the same repetitive work. If you follow along, I would still encourage not to skip this refactoring, because it's a nice practice. EnrollCommandHandler, enroll controller action, transfer command, transfer command handler. This one is a bit large, but that's because the controller action itself is large, too. Replacing it with these two lines. Good. The last one is disenroll. Here's a command for it, a command handler, and the use of them. Very good. As you can see, the whole controller is finished. We don't need these three fields anymore, so I can remove them from here, and from here, and we don't need the UnitOfWork either. Cleaning up the using statements, and let me get rid of these comments. It's now obvious which of the controller methods is a command and which of them are queries, because all our messages have command and query postfixes, and in general, I don't like comments, I only employ them if they are absolutely needed to convey something that is impossible to convey with the code itself. The last thing here is these repetitive transformations of the result instance into http responses. We can extract them out, too. Let's create a new method in the base controller called FromResult. It will accept a result, and I'm copying the implementation from here. Good. Replacing this line here with the new method, and replacing the rest of them, too. Perfect. The last thing here is to register the command handlers in the startup class, like this, and let's make sure the application is still working. I'm disenrolling Alice from the second course, providing some comment, enrolling her back to Microeconomics, and let's also transfer her to this course, with the A grade. Perfect. As you can see, everything is working just as before.
Recap: Introducing Commands and Queries

In the last three demos, we refactored the code of the student controller. Now it uses explicit command and query objects to do its work, and because of that, the controller itself has become a thin wrapper on top of these commands and queries. In theory, you could even remove this controller. There's not much value in it anyway. Here you can see all it does is it creates commands and dispatches them. In practice, however, there's just too much of internal ASP. NET wiring that is tied to the presence of controllers. For example, the HTTP method, such as GET or POST, and URL routing. So, it's impossible to get rid of controllers entirely, but you won't be using them for anything other than this ASP. NET wiring stuff. All the application logic will go to the commands and query handlers themselves, which is a good thing. Note once again how we leveraged the ASP. NET dependency injection mechanism for the resolution of command and event handlers. Our Messages class uses the internal ASP. NET's service provider to resolve the command handler of the required type. For that, it constructs the ICommandHandler interface using the type of the command argument. Very convenient, indeed. The drawback of this approach is that we now need to manually configure all those handlers one by one in the startup class so that the service provider knows which class it needs to create to resolve a given interface. You can mitigate this issue by using a dependency injection library. Most of such libraries can scan your assemblies for all implementations of a particular interface, so that you don't have to register them manually, or you could write this functionality yourself. It's just a couple lines of code to do this using reflection. I'll leave it to you as an exercise. Note the difference in the semantics between the command handler and the query handler. The command handler returns an instance of the Result class, a special class that represents either a success or a failure of the operation. It also allows you to pass an error message in case of a failure. The command doesn't return anything other than this confirmation of the operation's success. It follows perfectly the semantics of a command. A query, on the other hand, does return an actual result. It's a bit confusing here, because the difference in code is just a single letter, but there is actually a big difference between Result and TResult. As I mentioned, the result class is just an acknowledgment of the operation's success or failure, whereas TResult is a type parameter and it can be of any type you'd like. In our sample application, the single query we introduced returns a list of student DTOs. Note that you can make this return type an instance of the result type, too, and that would be a result of TResult. This is useful when you know that the query operations can fail, too. For example, due to issues with a database connection, or if the query parameters can be invalid and you decide to prohibit the execution of invalid queries.
Summary

In this module, we refactored our application towards using explicit commands and queries. All our application code now resides in separate command and query handler classes. We also introduced a unified interface for all our command and query handlers. This will help us in the next module when we'll be working on decorators for them. ASP. NET core has a great dependency injection mechanism out of the box, and we've leveraged that mechanism for the handler resolution. Let's recap the key points of this module. First, the difference between commands and queries in CQS and CQRS taxonomies. In CQS, which stands for the Command-Query Separation principle, a command or a query is a method that either mutates the state of the application and doesn't return anything to the caller, or does return something, but doesn't leave any side effects in the application state. In CQRS, which stands for Command-Query Responsibility Segregation, a command or a query is a class that represents what you can do with the application. This class has a handler with a single Handle method, which also follows the CQS principle. It means that the Handle method of a CQRS command handler doesn't return anything to the caller, aside from the confirmation of the operation success or failure, and the Handler method of a Query handler doesn't incur any side effects. As you can see, the definitions are quite interconnected. You can view a CQRS command as a serializable method call and a command handler as a regular ASP. NET controller with just a single method that is a command in the CQS sense. All messages in an application can be divided into three categories: commands, queries, and events. A command is a message that tells the application to do something, a query is a message that asks the application about something, and an event is an informational message. It tells external applications about some change significant to your domain. It's important to properly name all three types of messages. Commands should be in the imperative tense. They are telling the application to do something. This naming guideline also implies that your application can reject this message. On the other hand, an event is a message that should always be in the past tense. That's because it states a fact, something that already happened in the past. It also implies that there is nothing the application can do about this message. The change it informs about has already taken place. Queries should follow the same naming guideline as commands, except that in the vast majority of cases, they should start with Get. Always use the ubiquitous language when naming the messages. It will allow you to avoid the fallacy of the CRUD-based thinking that we discussed in the previous module. We discussed where to place commands and queries in the onion architecture. They should reside in the core domain layer, the same place the domain events belong in. The only difference between commands and events is that the commands follow the push model, whereas the events follow the pull one. It means that it's the other applications that trigger the commands, and it's your application that triggers the events. We talked about the differences between commands and DTOs. In a publicly exposed API, you need to have both. DTOs, data transfer objects, allow you to achieve backward compatibility, whereas commands allow you to explicitly state what your application can do. Don't combine these two responsibilities unless you don't need backward compatibility for some reason. In the next module, we will discuss how to implement decorators upon the command and query handlers. It can be a very powerful mechanism that allows you to achieve great flexibility with little effort and maintenance cost.
Implementing Decorators upon Command and Query Handlers
Introduction

Hi, my name is Vladimir Khorikov and this the CQRS in Practice course. In the previous module, we introduced explicit commands and queries in our code base and handlers for them. In this module, I will show you how to enrich those handlers in a simple, yet very powerful way. You will see how the groundwork we have laid so far, allows us to easily introduce cross-cutting concerns in our application.
New Requirement: Database Retries

Let's say that we've got a new requirement. Our database goes offline from time to time, because the connection between the application and the database is unstable, and so we need to implement a retry mechanism to deal with this issue. Let's see we could do that. Here is the EditPersonalInfoCommandHandler. One way to implement this requirement in this particular handler would be to write something like this. Try to commit the transaction, say, three times. For that, we need to wrap the Commit method call into a try-catch statement. This commit method is where the database transaction gets committed and all the SQL queries are executed. So if there is any connection interruption between the application and the database server, it will show up in this line. This approach sounds plausible at first, but unfortunately, it wouldn't work. First of all, when a database connection breaks up, the connection that our application keeps hold of becomes unusable. You cannot simply call the Commit method one more time. Even if the database goes online at that time, ADO. NET will throw an exception saying that this connection is broken, and that the recovery is not possible. So, you need to instantiate a new database connection on each attempt. Another issue here is that this Commit method is not the only one that reaches out to the database. We also have this line here that retrieves the student. If for some reason, the student is no longer in the database between the first and the second attempt, you need to somehow show this fact and not just blindly keep retrying the operation. And so it turns out that the only reliable way to implement the retry is to re-run the command handler as a whole. Re-run the full code of the handle method, which as you can see, is quite verbose, and in addition to that, it means a lot of code duplication. If we want to implement such a retry in any other command handler, we won't have any choice other than copying and pasting this loop with the try/catch statement in all of our handlers. Fortunately, there is a better way. As I mentioned in the previous module, we can take advantage of the unified interface all our command and query handlers implement, and introduce decorators on top of them. Let's implement such a decorator and after that, discuss what that is in more detail.
Introducing a Database Retry Decorator

Alright, so let's start with our first decorator. I'm creating a new folder for it, called Decorators, and a new class, Database Retry Decorator, making it public and sealed. Good. Just as a regular handler, it should have a type argument, TCommand, and it also should implement the ICommandHandler interface with that type argument. The type argument itself should be constrained to the ICommand interface; implementing missing members, good. Inside the handle method, we will introduce a for loop that will retry the action for three times, and inside of it, there will be a try-catch statement, like this. This line is where we will do the retry itself. So, how to do the actual retry? We need a reference to the handler this decorator decorates, and how to get it? We can request it to be injected into the constructor. So, let's create one. Accepting another ICommandHandler of TCommand, saving this handler to a private field, and making this field ReadOnly. Now we can use this handler here, in the try-catch statement, handler. Handle, and return the result to the caller. So the idea here is to catch any exception that pops up from the handler and then, if it's related to the database connectivity issue, retry the same handle method once again, and we need to do that only if the number of attempts doesn't exceed three. Checking that the exception is indeed a database exception, if any of these conditions doesn't hold true, re-throw the exception. The check as to whether the exception is a database connectivity exception will look like this. It should always have an inner exception with one of these two strings. Alright, so now we need to register this decorator, and that configuration resides in our Startup class. Here you can see, the mapping is currently set on the type level. We are specifying the type of the class that ASP. NET should instantiate whenever we request this interface to be injected somewhere, but how to decorate this class with our new decorator? For that, there is an overload of this method that accepts a factory. Here it is, a delegate that returns an instance of the ICommandHandler. This factory itself accepts an instance of the IServiceProvider interface, the same interface we are using in the Messages class to resolve our handlers. So, the delegate will look like this. It will instantiate the decorator, and also instantiate the handler and pass it as a parameter to the decorator's constructor. The handler requires an instance of UnitOfWork, and that is where the service provider will be helpful. We can use it to resolve that instance, or any other dependency we registered in Startup, for that matter. This is useful because it allows you not to think about such things as lifetime management of those dependencies. Just call the GetService method and the dependency injection container will do everything else for you. Alright. We have a compilation error here, saying that not all code paths return a value. Let's fix it by throwing an exception here. Another issue in this class is that we are using the magic number three here and here. It's better to extract it into a config file. So let's create one. Config, making it public and sealed, and adding a single setting we have so far in our application, the number of database retries. Creating a constructor, good. In the startup class, I can instantiate the config with the number of retries set to three. Note that in a real-world application, you should deserialize this instance from the appsettings. json file. I'm doing it manually here for brevity, and now I can add this instance as a singleton to our dependency injection container. Going back to the retry decorator, adding this class as a dependency to the constructor, saving it to a private field, making it ReadOnly, and replacing both numbers with the value from the config. Good. Because we changed the signature of the decorator's constructor, we need to update the factory method here as well, by adding the config instance here, and actually, it's better to be consistent with regards to such dependencies, and use the service provider whenever you can. So, I'm replacing the config instance with another GetService call, like this. Another thing we need to modify in the handler implementation is we need to replace the UnitOfWork dependency. As we discussed previously, when the database becomes unreachable, you cannot reuse the existing connection, even if the database goes back online. And here you can see that the UnitOfWork class is just a wrapper on top of NHibernate's session, which itself contains an ADO. NET database connection as part of it. What we need to do instead is we need to inject the SessionFactory class in our handler and then instantiate the UnitOfWork manually. You can see in the startup class, the session factory is a singleton. There is only one instance of it in our application, and we can use this instance to instantiate all our units of work. Alright, so let's replace the UnitOfWork with SessionFactory, save it to a local field, mark as ReadOnly, and we can now get rid of the UnitOfWork class in the constructor. What we can do instead is we can instantiate a new UnitOfWork in the handle method itself using the SessionFactory, and replace these two variables with it. Good. In the startup class, we need to fix the compilation error, provide an instance of SessionFactory instead of UnitOfWork. Very good. Let's test our new functionality. I'll put a breakpoint here in the EditPersonalInfo command handler, and run the application, modifying Alice's name to Alice 2. Alright, the breakpoint is hit. Now, we will put another breakpoint in the decorator, and to emulate the database connection loss, I'll go to the list of Windows Services and stop the SQL Server. Continuing the execution, and you can see that the breakpoint is reached in the catch statement. We've got an exception, because the ORM cannot access the database anymore. I'm allowing the program to execute the if statement. Very good. The exception is not re-thrown, which means that it is successfully categorized as a database exception. Alright, let's bring the database back online, and continue the execution flow, and you can see on the UI that the name is changed to Alice 2. It means that our decorator re-ran the handler and got a successful result with the second attempt. Perfect. And before we move forward, let me fix a couple things in the code. First, we don't need this check in the for loop. It already exists in the if statement inside the catch block. So no need to duplicate it here. Once I remove it, Resharper says that we don't need to throw an exception here anymore either. So I can get rid of this line, too. And the second issue is this typo here. I missed the E letter in the word database. Very good.
Decorator Pattern

In the previous demo, we implemented our first decorator, which detects database connectivity failures and re-runs the same command handler several times, until it either reaches the limit of three attempts or gets a successful result, meaning a result without an exception. Now that you saw the decorator pattern in action, it's time to discuss it in more detail. So, what is it? Decorator is a class or a method that modifies the behavior of an existing class or method without changing its public interface, and thus, without affecting the clients of that class or method. In our sample application, the decorator implemented the same ICommandHandler interface all other commands implement, but instead of being one of the true commands, so to speak, it enhanced their behavior with additional logic, and when doing so, it didn't require us to modify any of the clients of this command handler. The only client of the handler is currently the Messages class, and you can see, we continue to generate the same ICommandHandler interface when resolving the handler. No changes were needed in this class, and still, instead of the actual EditPersonalInfoCommandHandler here, we get a decorator instance that wraps our handler and adds its own functionality on top of it. To make this all work, we registered the corresponding command handler with a factory method. This factory method first creates the handler, and then passes it to the decorator. This is a very powerful technique. It allows you to introduce cross-cutting concerns without a single bit of code duplication. All the orchestration is gathered in a single place, our decorator, and we can reuse it in any other command handler we'd like. Just think about how many repetitive code we have eliminated, thanks to this approach. The only thing we needed to do for that is update the configuration in the startup class. Everything else is handled by the plain ASP. NET dependency injection infrastructure. This approach also allows you to adhere to the single responsibility principle. Because we moved the retry logic into a separate class, we are able to keep the handlers themselves simple and focused on the actual business use cases, not the technical details such as database connectivity issues. We can separate these two concerns. Moreover, as you introduce more and more decorators, their composition becomes increasingly rich in terms of functionality, and still, each separate element of this composition remains very simple. This is, by the way, the end goal and the essence of most programming techniques, using simple building blocks to compose as complex software as needed, without increasing the complexity of each individual part of that software, and in this particular case, you can see one of the finest examples of it. The implementation we introduced is actually very similar to what ASP. NET Core does with its middleware infrastructure. We'll discuss this similarity in more detail later in this module.
Introducing Another Decorator

Let's say that we received another requirement, implement audit logging for our EditPersonalInfo command handler. The stakeholders want to have a track in our audit log of the changes people make to their personal information, and that's on top of the retry behavior we have at the moment. Thanks to the mechanism we've developed, we know how to deal with this request, by adding a new decorator. So, let's do that. I'm adding a new class to our decorators folder called AuditLoggingDecorator. Just as our first decorator, it will implement the ICommandHandler interface. Implementing missing members, and injecting a handler in the constructor. Now, in the Handle method, we want to somehow log the incoming command to have the audit trail. One way to do so would be to serialize it into a JSON, and then log the resulting text, and of course, I'm using Console. WriteLine here just for brevity. In a real-world application, you need to inject a logger into the decorator the same way we did with the config instance in the previous decorator, and use that logger instead. Finally, we can execute the handler. In order to use this new decorator, we need to adjust the Startup class, add it to the existing method factory, and pass the decorator we already have as a parameter. Note that the order in which you wrap one decorator on top of the others does matter. Should we choose to inject these two instances the other way around, AuditLoggingDecorator inside the DatabaseRetryDecorator, we would be an audit record on every retry, which is obviously not what we want here. So pay attention to the ordering here. Let's run the application to make sure the new functionality works. Changing Alice's personal info, okay, and let's go to the output window, switch to the Web Server source, and here it is, our command that was successfully serialized and displayed along with other log messages.
Streamlining the Decorator Configuration

As you can see, the decorator pattern along with the command and query handlers is a powerful mechanism that allows us to create complex behaviors using simple building blocks. However, there's one drawback to this implementation. As you can see here, in the Startup class, we now have quite a convoluted configuration code, and that's with just two decorators that we are using on just one command handler. As we continue to develop our application, this code will become too verbose very quickly. We could extract it into a separate method, of course, but there is a better way. What we can do instead is we can define the decorators with attributes on the handler classes themselves, like this. This will allow us to apply a declarative approach where we could annotate the handlers we would like to enrich with additional functionality, using just a single line of code. And at the same time, this will allow us to get rid of all this configuration code. So, how can we do that? Let's see. First of all, we need to add the attributes I used in the EditPersonalInfo command handler. Here is the DatabaseRetryAttribute. It's a standard attribute with nothing particular interesting about it. The only thing to note is that its usage is restricted to classes. So, we can only put it on top of classes, but not methods, and here is the other one, AuditLogAttribute. Let me extract it to another file, good. Now these attributes can be used in the handler. I just need to import the corresponding using statements. Now it is time to map these attributes to the decorators, or more specifically, properly register our handlers, taking into account the decoration attributes. For that, I will create a new class, HandlerRegistration, make it static. Good. The only method this class will contain is the AddHandlers method, which would be an extension method on top of IServiceCollection. We could make it a regular static method instead, but it's a nice convention to always define methods that register ASP. NET services as extension methods. Here's what its usage will look like in the Startup class; just a single method instead of the bulk of code we had here previously. Alright, what would be the implementation of it? I'll go over this implementation with you, and it might look quite complicated, but don't worry if you don't fully understand it. It's mostly plumbing code, anyway. Also note that there are dependency injection libraries out there that can do such registration for you. Okay, the first step would be to scan the assembly with the domain model for all command and query handlers in it. Here you can see we are getting all types from the assembly and filtering them so that they only contain the types that implement a handler interface and types that end with Handler to exclude all the decorators. This IsHandlerInterface method looks like this. It checks if the interface is either an ICommandHandler or IQueryHandler. For each command or query handler type, we call this AddHandler method, and here's its code. You can see it retrieves all attributes attached to the type, converts them to decorator types, adds the type of the handler itself, and creates a list out of that. I called this list Pipeline. So for our EditPersonalInfoCommandHandler with these two attributes, the list would look like this. The handler is created first, because that's what will do the actual work. On top of the handler, our code will create the AuditLoggingDecorator, and on top of that, the DatabaseRetryDecorator. Here's how the ToDecorator method looks. It's just a switch that matches all our attributes with the corresponding decorators. You need to add new attributes and decorators here. This line retrieves the interface the handler implements. So for the type EditPersonalInfoCommandHandler, the interface would be ICommandHandler of EditPersonalInfoCommand. This method will build a factory method out of all the decorators and handlers we found, and this one will register it in the service collection. Here's the BuildPipeline method. It first gets the constructors of each of the pipeline type, creates this delegate, and returns it. Inside the delegate, we need to iterate through all the constructors. For each of them, get a list of parameters they require, resolve them somehow, and then invoke the constructor and create an instance of the type in the pipeline. This way, we are able to first create the handler itself, then use it to create a decorator on top of it, and so on for each decorator in the pipeline. The GetParameters method will create a resulting object array and for each parameter, will call this method. This method will see what is the type of the parameter and, if it's a handler interface, will use the type from the pipeline we have instantiated so far. Otherwise, it will use the dependency injection container to resolve it. Alright, that's pretty much it. Now in the Startup class, I can remove all these lines. The handler registration class will do the work for us. Now, in order to enrich any of our command handlers, what we need to do is just put an attribute on top of it, and that's it. Let's run the application to see if it's still working. I'm editing Alice's personal info, good, transferring her to Microeconomics, very good. All the functionality works just as before. And just a couple of last edits. In order to the retrial logic to work, we need to replace the UnitOfWork with the SessionFactory, because, remember, we cannot reuse the same UnitOfWork when re-running the handler. So, here, I'm just doing the replacement real quick. UnregisterCommandHandler, and the rest of it. Good. Now that we don't use the UnitOfWork class, we can remove it from the services. Clean up the using statements, and you can see, we have simplified the Startup class a lot.
Recap: Streamlining the Decorator Configuration

In the previous demo, you saw how we streamlined the decorator configuration. Now, in order to enrich a handler, the only thing we need to do is put decorator attributes on top of it. All the configuration is now simple and declarative, and also cohesive. Because the handler and its attributes reside close to each other, it's really easy to see how each of the handlers is configured. No need to refer to the Startup class for that. Note once again that the ordering of such attributes is important. The decorator whose attribute we put on top will be executed first, the second one after that, and so on, down to the handler itself. In this particular case, you can see that the database retry decorator goes first and the audit log one goes after it, which means that if we get a database connectivity issue, each of the attempts will be logged separately. This is not what we would like to happen, so we need to re-order the attributes, like this. This way, even with several retries, the handler invocation will be logged only once. And again, although we have introduced our own implementation of the decorators, you can very well use an existing dependency injection library for that, such as, for example, Simple Injector.
Decorators vs. ASP.NET Middleware

At this point, our command and query handlers and the decorators on top of them look a lot like ASP. NET middleware infrastructure. If you look at the single ASP. NET middleware class we currently have in our project, the ExceptionHandler middleware, and compare it to a decorator of ours, they look very similar, and indeed, they implement the same pattern, the decorator pattern. The only difference is that the middleware doesn't rely on explicit interfaces like our ICommandHandler. ASP. NET uses reflection to find a corresponding method to call in the middleware classes, and that is this Invoke method. Other than that, both of them accept the next item in the pipeline as an input parameter, and both of them put their own logic on top of it. The exception handler converts all unhandled exceptions into 500 responses, and the decorator reacts to exceptions from the database by retrying the same handler. This similarity is no coincidence. When building ASP. NET Core, Microsoft has taken into account all the experience the industry has accumulated in this area and rolled out a simple, concise, and overall well-thought-out solution. As I said, the middleware class implements the same pattern as our decorator classes, and if we continue the analogy, the controller classes in ASP. NET correspond to our command handlers. At this point, you might ask, if ASP. NET core in general, and the middleware infrastructure in particular are so good, why do we need our own decorators at all? Why not just use what we already have from Microsoft? And that would be a great question, and indeed, some of the features that people traditionally advised to implement using command handler decorators within the CQRS pattern now can be implemented using plain ASP. NET middleware; however, it's still beneficial to implement some of the cross-cutting functionality using the decorators infrastructure we developed in this module. There are three benefits to this. First, you are getting additional control over how this infrastructure works, and that could be quite helpful at times. Second, this provides the separation of concerns between ASP. NET and your application, which allows you to reason about them independently and therefore with less effort. As a result, such separation of concerns allows you to reduce the maintenance costs of your application. And third, the decorators we've developed are easier to apply selectively. The issue with ASP. NET middleware is that it's hard to tell ASP. NET to which controllers and API endpoints to apply them. You will need to manually analyze the URL of the incoming request and decide whether to apply the middleware logic to it or not. The resulting code is often quite confusing and non-intuitive. Attributes like those we introduced previously are much easier to work with. As an alternative, you could employ ASP. NET action filters instead of middleware, but they come with a lot of framework-related concerns, which again, don't help with the separation of concerns between your application logic and ASP. NET wiring. So, in terms of flexibility and code simplicity, the decorators we've developed are a better choice. Overall, middleware is good for cross-cutting concerns that are ubiquitous and ASP. NET-related. The exception handler is a nice example of here. It is ubiquitous, because we want it to run on every controller in our application. We want it to handle all unhandled exceptions, and it also takes care of ASP. NET-related concerns. It converts all unhandled exceptions into 500 responses, which correspond to internal server errors. Everything that is not ASP. NET-related is best implemented using our own decorators. They are more flexible, they are not bloated with ASP. NET concerns, and you have a full control over them. Here are a couple more examples of decorators that you could need to implement. Caching. This would be a decorator on top of IQueryHandler. In our application, it would be useful for caching some of the most used and expensive search queries, and thus offload the pressure from the database. Transaction handling. Currently, we encapsulate the work with both the database connection and database transaction in our UnitOfWork class. We automatically open a transaction when opening an NHibernate session. And you can do the same if you are using Entity Framework Core, but if you would like to execute some of your handlers without a transaction, a separate transaction handling decorator would be helpful here. With it, you can easily mark which of the command handlers you want to execute inside a transaction.
Command and Query Handlers Best Practices

Alright, we are almost done with this module. The last thing I'd like to talk about is some of the best practices you could employ when implementing the CQRS pattern. The first one is how to organize commands, queries, and their respective handlers. It's often the case that as you develop your application and refactor it, you need to re-organize some of the commands and handlers, and maybe even remove some of them. Because commands and handlers are represented by different classes, and because they usually reside in different files, it's also often the case that when you delete a handler, you forget to delete the corresponding command or query, and so you can end up with some orphaned classes, some commands or queries without their handlers. Fortunately, it's quite easy to avoid this. You can nest the handler class inside the command class. So, take for example, our EditPersonalInfoCommand handler, and paste it here, and that's it. You can even make the handler non-public. Let's quickly do that for all our query and command handlers. Alright, here's the end result of all of this. I've put all of our commands into separate files, nested the handlers inside of them, and also grouped the commands into a separate folder. Now if you decide to remove a command handler, you will be able to immediately remove the commands, as well because it resides right there in the same class. This helps with discoverability and code cohesion, too. Also note that all the handlers are made internal, and so they don't even comprise the public API of this assembly. At the same time, our handler registration class can still find them all because it scans this assembly using reflection, and so for our implementation, it doesn't really matter if the handler classes are public or internal. Alright, the second best practice I'd like to talk about is this. Don't reuse command handlers. It's sometimes tempting to do so when you have two similar use cases. In such a case, you might decide to create two command handlers, and call one of them from the other one, like on this diagram. That is counterproductive, however. Here's an example. This is the UnregisterCommand handler. It deletes a student from the database. You can see that I have added these two lines to disenroll the student from the existing courses before unregistering them. Let's just assume that business wants us to do that for some reason. This implementation looks quite reasonable at first, especially if you consider that we don't just call the DisenrollCommand handler directly, but instead create proper commands and dispatch them via the dispatcher. The problem here is the misuse of commands. Commands should not beget other commands; that is contrary to the principle we discussed in the previous module. It's other applications that trigger commands, not our system. Our system reacts to those commands and produces domain events, and our system cannot create subsequent commands on its own. A command is a representation of what the clients can do with our application, and only clients can raise them. Alright, but how to reuse code then, you might ask? You have the domain model for that. Just extract the common code from the two handlers into a separate class, such as a domain service, and use it in both classes. This will have the same effect in terms of the code reuse, but without the downside of reusing the command handlers themselves.
Summary

In this module, we discussed how to extend command and query handlers without code duplication and while keeping the handlers themselves simple. For that, we used decorators. Decorator is a class that modifies the behavior of an another class without changing its public interface. In other words, it's a wrapper that preserves the interface of the thing it wraps. It's a very powerful technique. It allows you to introduce cross-cutting concerns to your application without code duplication. It also allows you to adhere to the single responsibility principle, as you are able to separate the code of the cross-cutting concerns from the code that implements the actual business-use cases. It's especially useful because you can chain multiple decorators together, and thus introduce increasingly complex functionality, while still keeping each individual one of them small and focused on doing only one thing. You also saw how we streamlined the configuration of the decorators. We implemented our own mechanism for dynamic registration of the decorators. With it, we are able to mark separate handlers with one or more attributes and thus specify which of them we want to enrich with what functionality. This allowed us to achieve a fantastic degree of flexibility. We also talked about the similarity between our decorators and the ASP. NET Middleware. They both implement the same decorator pattern. Use ASP. NET middleware for ASP. NET-related functionality, for example, for converting all unhandled exceptions into 500 error codes. Use the decorators for everything else. There are three benefits in using hand-written decorators over ASP. NEt middleware. Additional control over your code, separation of the application and ASP. NET concerns, and better flexibility. Finally, we discussed some best practices around working with command and query handlers. First, put command and query handlers inside their respective commands and handlers; and second, don't reuse command handlers; extract similar code to domain classes, and reuse those instead. In the next module, we will talk about Simplifying the Read Model. Stay tuned.
Simplifying the Read Model
Introduction

Hi, my name is Vladimir Khorikov and this the CQRS in Practice course. In the previous modules, we created explicit commands, queries, and handlers for them. We also introduced decorators on top of the handlers. All this allowed us to keep the code simple and maintainable, even after we faced more requirements from the stakeholders, such as the database retry behavior and audit logging. That was one of the three benefits the CQRS pattern provides, code simplicity. It is very important, probably the most important benefit, but it's not the only one CQRS provides us with. In this module, we will be exploring the Read model in the attempts to simplify it. I will show how it allows us to increase the application performance.
The State of the Read Model

Before we start the deep dive into the Read model, let me give you a refresher on the terminology. The CQRS pattern is about creation of two different models, one for commands and the other one for queries. The term Query model is the same as Read model, Read side, Query side, or just Reads. And the same is true for commands. The term Command model has the same set of synonyms; Write model, Write side, Command side, or just Writes. They are all about describing the two sides of the application in CQRS. So, when we'll be talking about the Read model in this module, just keep in mind that it's the same concept as Query model. Alright, having that out of the way, let's take a look at our Read model. It currently consists of only one query, GetListQuery. It asks our application about the students who are registered in the system, and it also allows for filtering them by a particular course, or a particular number of such courses. And here is the handler. What it does is it calls the StudentRepository, gets a list of student entities out of it, and converts them into DTOs. Let's also look at the repository. We have gone over it already in the past module, but let's reiterate real quick. The method first creates an IQueryable of Student. This IQueryable is a special interface in. NET that enables data querying.. NET provides extension methods on top of IQueryable, the same extension methods as for IEnumerable, such as, for example, where; but the implementation is very different. The extension methods on top of IEnumerable work with collections in the local memory. They pretty much just go over the elements in those collections with a simple loop. IQueryable is more complex. It has two properties, a query provider and an expression. An expression is a type that contains metadata about the request, and the query provider has access to the database and knows how to query it given the above expression. When you work with IQueryable, it doesn't get executed until you call a method that transforms the IQueryable into an in-memory collection. Examples of such methods are ToList, Single, First, and so on. In our case, it's the ToList method that triggers the evaluation of the IQueryable. When we call Query in this line, the resulting IQueryable is a description of what needs to be queried and how. No database call just yet, so far. And when we call Where here, this method just updates that description with new instructions. No database calls here, either. The database call only gets executed here, in this line. The support of the IQueryable interface, also known as LINQ, is quite good in such ORMs as NHibernate and Entity Framework. However, there are some limitations. For example, you cannot use it to filter an object by the number of elements in one of its collections. You can see that we delegate the filtration by the course name to LINQ. That's something that LINQ can take care of, but when it comes to filtering the students by the number of their enrollments, the ORM cannot help us with that. It doesn't know how to transform such a filtration into a SQL query. That's why we evaluate the IQueryable interface in this line. We just have to do that. This line allows us to load partially filtered objects into memory and then complete the filtration there, using the plain LINQ-to-objects provider, which just loops through all those in-memory objects. As you might guess, because of this limitation, the performance characteristics of the GetList method are not very good. Because we cannot delegate the filtration to the database, we transfer an excessive amount of data to the application server from the database when we retrieve a not fully filtered set of students. The second issue here is the N+1 problem. In order to complete the filtration in the memory, the ORM needs the rest of the student data, particularly this Enrollments collection. Because this collection is not loaded as part of the student set, they are loaded lazily one by one. when LINQ iterates through this collection in the memory. All this affects the performance quite badly. It's not noticeable here, of course, but in applications with high performance requirements and with large or even moderate amounts of data, that will be a problem.
Separation of the Domain Model

So, our Read model lacks the performance. How can we fix this? Well, that's what CQRS is all about. Remember, the core principle of CQRS is to have two models instead of just one. One model for Writes commands, and the other one for Reads queries. And that, in turn, allows you to optimize decisions for different situations. During the refactoring of our sample project, we have been gradually introducing this separation. Remember we had a single giant update method in the beginning of this course. We separated it into several task-based API endpoints. That was a segregation at the API level. After that, we introduced further separation when we defined explicit commands and queries and the handlers for them. Thus the split has penetrated into the Application Services layer, but even that is not enough. We need to go further and introduce the separation at the Domain model level as well. Look at the repository once again. The GetList method uses the same Domain model as the command handlers. It uses the same student entity, and that puts a restriction on what we can do with this method. As I said previously, LINQ doesn't provide enough functionality for us to fully utilize our database. The resulting SQL query is not optimal. So the use of the domain model limits our ability to write high-performant SQL queries, but it's also true the other way around. If you look at the GetList query handler, you can see that we are using these FirstEnrollment and SecondEnrollment properties, and if you look at the usages of them, this GetListQuery handler is the only place where they are being used. So what we have here is unnecessary over-complication of the domain model in order to fit the needs of the query side of our application. This is a very important point, so let me repeat it once again. The fact that we are using the same domain model for both command and query sides of our application leads to two things. First, it overcomplicates that domain model. The domain model becomes more complex than it would otherwise; and second, it hinders our ability to fully utilize our database. We cannot utilize highly optimized database queries in such an environment. And that is a perfect illustration of what you usually end up with when trying to fit the Read and Write responsibilities into a single model. You end up having a more complex domain model that handles neither of those responsibilities well. Reads and Writes are inherently different, and the best way to address this difference is to make it explicit in the code base. We have already taken some steps towards this goal, but as I said, the current state of the separation is not enough. We need to split our domain model as well. So, what this split should look like? Are we going to have two domain models now? No, it's actually not that. It's not as much of a split as it is an extraction. We are going to take the domain model out of the Read side entirely. The Read side then will not work with any domain model whatsoever. That's another important point, so let me elaborate on it. There is no need for a domain model within the Read side of the application. The domain model is only required for commands, not queries. One of the ultimate goals of domain modeling is achieving a high degree of encapsulation, making sure that when you change something in the system, all the data in it remain consistent, and no invariants are broken. But because we don't change anything on the Read side, there is no need for the encapsulation, and, by extension, no need in the domain modeling, either. The only thing the Read side needs to worry about is how to better present data to the client, and so you can just drop the domain model from the query handlers. You can get rid of all other abstractions, too. For example, you don't need an ORM here, either. You can write all the database access code manually, and that would actually be beneficial in many cases, because you would be able to use the database-specific optimization techniques that you wouldn't be able to use with a complex ORM, such as NHibernate or Entity Framework. Let's see how we can do that.
Simplifying the Read Model

Alright, we are going to get rid of the domain model entirely in the Read side of our application. Here is the query. And here is the repository method once again. We will no longer be using it when querying data from the database. So, because we won't be using the ORM here, we don't need the SessionFactory, either. What we need instead is a direct connection to the database that we'll be running our SQL queries against. And to open such a connection, we need a connection string. So let's create a new class, ConnectionString. The only purpose of this class is to contain that string and pass it around in our application. The reason why we are creating such a wrapper is because we cannot use the ASP. NET dependency injection container to inject raw strings; it has to be classes. If we go to the startup class, we can instantiate this class using the connection string from the config file, and register it as a singleton. Now, to avoid this duplication, let's switch the SessionFactory to using the new ConnectionString class, too. So, change the type of the input parameter to ConnectionString, change it here, too, and refer to ConnectionString. Value in this line. This modification will allow us to replace this registration with just a straight AddSingleton call. Very good. Going back to the query, let's inject the ConnectionString in here as well. Save it to a private field, and make that field ReadOnly. We are ready to open a connection, and that connection is gonna be SqlConnection. I just need to reference System. Data. SqlClient; good. Let me comment this out, and also, to simplify the work with the raw SqlConnection, I'm going to use Dapper. It is a light-weight ORM that is perfect for scenarios where you write SQL queries on your own. All this library does is it maps the results of those SQL queries to your custom types, which is really helpful avoid a lot of boilerplate code. Alright, here is the SQL query we are going to use. No need to dive deep into it. Just note that this SQL query does everything in one database roundtrip, which greatly increases the performance, and here is a private class that would hold the results of that query for us. Note that I'm making it private because it doesn't need to be exposed anywhere; it's just for the use in this particular method. Executing the SQL query using the filtration parameters from the query object, extract all identifiers out of the results, and for each of those Ids, select all the data from the collection, and transform it into a DTO. Add that DTO into the output list, and return it to the caller. So, here it is, our new implementation of the GetList query handler. We can now remove this, this method, and the GetList method in the repository. Perfect. Let's run the application to make sure we didn't break anything. And you can see that the list of students is successfully displayed on the UI. We are able to filter this list by the number of students, and by the course the students are enrolled in. Very good. Now, we can go to the student entity, and get rid of these two lines here. We have been using them for the sole purpose of querying the data from the database.
Recap: Simplifying the Read Model

In the previous demo, you saw how we simplified the Read side. It no longer uses the domain model, nor does it use NHibernate. We did the data retrieval manually, using a custom SQL query, and because of that, because we removed all those abstraction layers, our Read model has become just a thin wrapper on top of the database. On the one hand, it means that we now have to write all our SQL on our own, but on the other, it also means that we are not restricted by the abstraction layers. We can use as many database-specific features as we want, and that allows us to create a highly optimized and performant solution for our specific problem. Note that it's not a bad thing that the Read model is now tightly coupled to the database vendor, SQL Server in our case. Remember we discussed previously in this module that one of the main goals of building a domain model is encapsulation. Making sure that any change to our data remains internally consistent, and maintains all the invariants. In the Read side of the application, we don't modify anything. Because of that, we don't have to worry about the encapsulation, and we don't have to worry about the domain modeling itself either. Remember, Domain-Driven Design lives on the Commands side of your application. The Read side is just a thin wrapper on top of the data storage where you can do anything you want; of course, as long as you don't modify that data. Otherwise it wouldn't be the Read side. You can write complex and highly optimized SQL queries. You can use vendor-specific features, such as, for example, indexed views. By the way, we could really use one of those to speed up the calculation of the number of student enrollments. Heck, you can even use stored procedures. They are usually terrible, but only if you are using them on the Write side of the application. On the Read side, they are just fine. People often ask, doesn't such approach to the Read model make it anemic? And the answer to this question boils down to encapsulation. An anemic domain model means not encapsulated domain model. There is no need for encapsulation, if you don't modify any data. I talk about it in much more detail in my other course, Refactoring from Anemic Domain Model Towards a Rich One. Check it out for an in-depth discussion of this concern, including how anemic domain models relate to functional programming. Alright, let's go back to our recap. We have optimized the data retrieval. We now select only the minimum amount of data and we do that in just one database roundtrip. In other words, we have got rid of the N+1 problem. The N+1 problem is when we first selected the students, then their enrollments, and then all their courses, all in different database roundtrips. That is, by the way, another common concern people have; the N+1 problem and the use of ORMs, such as NHibernate and Entity Framework. The concern boils down to the fact that it's very easy to get the N+1 problem when using an ORM with enabled lazy loading. People then sometimes go ahead and propose to either disable the lazy loading, or not even use those ORMs altogether. However, while the problem indeed exists, there is no need for such drastic measures. Just don't use the ORM or the domain model on the Read side of the application. That would be enough to avoid the N+1 problem and related performance issues. As you saw in the previous demo, the CQRS pattern allows you to do exactly that. It allows you to optimize each model, Read and Write model, separately for the different requirements our application poses to them. We have also simplified the Commands side. We have removed the FirstEnrollment and SecondEnrollment properties that were previously used by the GetList query. Now our domain model focuses entirely on the processing of commands. It exposes less state, which is always a good thing. Also, our repositories now have very few methods aside from GetById, Save, and Delete. That's usually all the domain model needs, when you don't use it in the Read side of the application. You can see, there is just one such method, GetByName. And in fact, we can remove these repositories altogether. There is not much value in them in our current architecture, anyway. For example, here, in the RegisterCommand handler, we can just remove the two repository instances, and use the UnitOfWork here directly. We have Save, GetById, and Delete methods in the UnitOfWork class already, and we can create another method called GetCourseByName here, too, to accommodate this remaining query. No need for having a whole layer with repositories that don't do much. Overall, this is a common pattern that I see each and every time when people start applying the CQRS pattern in their code bases. It is impossible to create an optimal solution for searching and processing of transactions utilizing a single model, and because of that, both Reads and Writes benefit from the separation. Writes benefit because you get rid of a lot of code in the domain model that is not used for data modifications, and Reads benefit because you are able to optimize the data retrieval. A common approach I recommend here is this; use a fully-fledged, highly encapsulated domain model with a big ORM such as NHibernate or Entity Framework in the commands, and use handwritten SQL with no domain model in the queries. This concludes our discussion of the second goal of CQRS, performance, but before finishing this module up, let's talk about one more thing.
The Read Model and the Onion Architecture

You might have noticed a subtle inconsistency in the way we implemented the query handler in the past modules. This inconsistency relates to the onion architecture. We discussed previously that queries, just as commands and events, are part of the core domain model. They represent what the clients can ask of our application, but the same time, look at how we defined our query. Can you see what's wrong with it? Take a moment; I'll wait a couple seconds. Ready? Okay. The problem with this query is that it implements the IQuery interface of the type List of StudentDto. Where does this StudentDto belong on the Onion Architecture diagram? It belongs in the outermost layer of our application, because it's the data contract that we define for our clients. This DTO has nothing to do with the domain model, but we still used it from the Query, and by doing so, we have violated the domain model isolation principle. This principle states that each layer in the onion architecture, each class in that layer, can either refer to classes from the same layer or to classes from the inner layers. It cannot refer to classes that reside in layers upper in onion, but that's exactly what we are doing here. What we had to do instead to avoid this violation is we had to make the query return a list of students, not student DTOs, and then convert them into DTOs in the controller. This way, we would have preserved the domain model isolation. However, that's a minor issue and it's minor precisely because of what we did in this module. We no longer use the domain model in our queries. We introduced the separation on the domain model level, and because of that, our queries no longer reside in this onion. The whole domain model is now dedicated to the commands only. Queries now reside in their own Read model that is not connected to the onion architecture anymore; it's a thin wrapper on top of our database.
Summary

In this module, we discussed how to simplify the Read model. We refactored our application so that the Read side of it no longer uses the domain model. It also doesn't use NHibernate anymore. We wrote the data storage code manually, using the raw SQL. We introduced the separation at the domain model level. This separation allowed us to both simplify the command side and optimize the query side of our application. The command side has become simpler, because the domain model no longer contains the code used by the queries, and we can even get rid of the repositories entirely, as the most complex methods have migrated from them to the query handlers. This would allow us to simplify the commands even further. The query side has become optimized because we were able to use database-specific features to retrieve only the minimum amount of data, as well as do this in a single database roundtrip. In other words, the Read model has become a thin wrapper on top of the data storage. This is beneficial because there's no need for encapsulation here. The Read model doesn't mutate the data, and thus it cannot violate any invariants in the application. We also talked about the Read model and the onion architecture. There is a subtle inconsistency in how we structured our queries. It looks like they shouldn't have used the DTOs, because those DTOs belong in outer layers of the onion. However, if you consider that queries are no longer part of the onion, they are no longer part of the domain model, then the inconsistency vanishes. The only messages that remain being part of our core domain are commands and events. In the next module, we will continue exploring the Read side of our application further. We will talk about introducing a separate database for it.
Introducing a Separate Database for Queries
Introduction

Hi, my name is Vladimir Khorikov and this is the CQRS in Practice course. In this module, we will talk about introducing a separate database for the queries, for the Read side of our application.
Meet Scalability

Throughout this course, we've been gradually introducing more and more separation in our application. We started with splitting the API endpoints. That was a separation at the API level. Then we created explicit commands, queries and handlers for them. This way, we have split our Application Services layer. After that, we proceeded to the domain model. We have removed all references to the domain model from the Read side of our application. As we progressed with the separation, we gained more and more benefits out of it. For example, the separation at the API and the Application Services layer gave us simplicity. After extracting task-based API endpoints out of a single giant CRUD-oriented one, both our controllers and the user interface became simpler and started to make much more sense from the user perspective. Introducing explicit commands and queries allowed us to bring in the concept of decorators which, in turn, helped us implement the cross-cutting functionality with very little effort. The separation at the domain model level gave us performance. As we removed the domain model from our queries, it became much easier to optimize them, because we were no longer restricted by encapsulation and unnecessary abstractions. It also gave us additional gains in simplicity, as we were able to focus the domain model solely on command processing, and remove from it all code related to queries. As you might have guessed already, we will not be stopping here. We will proceed with the separation and continue straight to the data level. We will introduce a separate database for queries, and that will provide us with the third benefit of CQRS, scalability. Let's take a minute to discuss what that is and why it is important. So, what is scalability? It is similar to performance, but it's not the same thing. With performance optimizations, you can achieve pretty good results, but you are still bound to the resources of a single machine. There is only so much you can get out of just one server. There is a physical cap on how many resources it can provide. Scalability takes place when a single server becomes not enough. It allows you to utilize the resources of several, and potentially, an unlimited number of servers, and thus meet the demands of the ever growing user base. In a typical enterprise-level application, there's a large disparity among the operations this application handles. Among all those create, read, update, and delete operations, the one that is used the most is usually read. There are disproportionately more reads than writes in a typical system, and so it makes a lot of sense to scale them independently from each other. The separation at the data level allows you to do exactly that. Even if you still have a single server that handles all the commands, by creating a cluster of several servers just for queries, you will drastically increase the application output. Because processing of commands and queries is fundamentally asymmetrical, scaling these services asymmetrically is beneficial, too. The synchronization between the databases of the command and query sides is a topic that we will discuss separately, but in general, the Commands database is a master storage for all data in the application. The Queries databases synchronize with that master storage, and then serve the reads on their own. This way, they offload the pressure from the commands. Note that although the gain in the application output is quite significant, there's still a limitation here. You are still bound to the resources of the single machine that runs the commands. CQRS by itself doesn't provide any guidance in scaling of the commands, only the queries. To go even further, we would need to introduce sharding, but it's outside the scope of this course. So we won't be diving into that topic here. Keep in mind, though, that it's quite rare that a typical enterprise-level application would need to shard data in the commands, because the number of reads is usually one or even several orders of magnitude larger than the number of writes. The introduction of a separate database, or multiple databases, for reads is usually more than enough to meet all possible requirements to such an application.
Separation at the Data Level in the Real World

We have touched upon this topic already in the first module, but it's important enough to reiterate it once again here. There are a lot of examples of this type of CQRS in the real world, where you keep separate databases for reads and writes, and you don't even necessarily need to create and maintain a separate database for that yourself. If you ever introduced an indexed view in your SQL Server, that's an example of the separation at the data level; the one that we just talked about. Here, the database server provides all the required infrastructure, and so you might not even think about it as being a form of CQRS, but it still is, nonetheless. Another example is database replication, when you have one master database and several replicas that get all the updates from the master node automatically. Here, the database software also does all the work for you so that you don't need to worry about the data synchronization, but the principle is exactly the same. You have a single node that is a master of all the data in the system, and you have one or more replicas that pick up all the updates from that single node and then can be used to serve the reads. This way, they offload the pressure from the master database, and allow you to scale the overall system output. So, one master node for writes, and several replica nodes for reads. Finally, another common example is ElasticSearch. It's a full-text search engine that works by indexing data, usually from a relational database, and providing rich capabilities to query it. So you can see that, in order to achieve the third benefit, scalability, you don't necessarily need to implement the separation at the data level yourself. You can leverage an already existing software, which can be just as good, but at the same time, saves a lot of effort.
Designing a Database for Queries

Database features that work out of the box can be easy to use, and you should definitely consider them first before rolling out your own implementation; however, they are quite limited in the type of functionality they provide. For example, with the out-of-the-box database replication, despite the fact that you will have separate physical databases for queries, those databases will still have the same structure, but as we know, reads and writes have very different needs, and the same database structure, just as the use of the same unified domain model, might not fit well the read side of our application. And indeed, if you look at our SQL query, you can see that although we squeezed everything we could out of it in terms of performance, there could be done much more if we just had a more appropriate database schema. Here it is, the schema of the commands database. We could denormalize it into something like this. Yes, that's the whole database; just one table with this seemingly terrible structure. So, what's the reasoning behind such a drastic change? This database structure is a much better fit for the needs of our Read model. Here's our UI once again. What data do we need to compose such a UI? Well, first of all, we need the students themselves, their names and emails. Then we need the information about their first and second enrollments; the name of the course, the grade, and the number of credits they will get. We can only have two enrollments currently, and we also need the total number of courses in order to support our filtration functionality here. And if you look at our new database, that's exactly the information we are having here, in the student table. It has the name and the email of the student, the total number of enrollments, which can be either zero, 1, or 2 at this point, and we have the information about the first and the second enrollments; the course name, the number of credits, and the grade. We have adjusted the schema of this database to perfectly match the needs of the Read side of our application. No need to assemble the required information with a complicated SQL query anymore. All we need is now represented by this single, flat table. And let's see how we can simplify the SQL query. Here is the old one once again. You can see it gathers the information from three out of four tables in the main database; student, enrollment, and course, and here is how the SQL will look should we introduce the new database, much simpler and much more concise. Again, we can do that because all the information the Read side needs resides in a single table with the schema that matches our needs perfectly. Let's see how it can look in practice.
Creating a Database for Queries

I've already created the database with the schema we discussed off-screen. Here it is, with just a single table, Student. This is the table's columns, and this is the data inside. I've added a couple of rows manually off-screen as well. And this is the script that you can use to create the database. You can find it on the GitHub page or among the course materials on Pluralsight. Alright, we now have two databases; one for reads and the other one for writes. And so we need another class to represent the this new database connection string. Let's go to our existing ConnectionString class, and add a new one, QueriesConnectionString, and rename this one to CommandsConnectionString. Note that you could just add another field in the already existing ConnectionString class, so that it would contain information about the both databases, but that would be a violation of the interface segregation principle; the I letter among the SOLID principles. That's because you never actually need both of them. You don't need to create connections to both databases at the same time. It's also much less error prone to have two classes instead of just one. This way, you will not be able to accidentally mistake one for the other. For example, pass a connection string for queries where the connection string for commands is required. Okay, now we need to add the actual connection string, the string itself. That would be our config file. I am naming it QueriesConnectionString, and specifying the database name. In the startup class, I'm changing the old class name to the new, instantiating a connection string for queries, and registering it in the services as another singleton. Let's fix the compilation errors. The first one is in the session factory. We need to turn the old ConnectionString into the CommandsConnectionString. That's because we will be using our ORM only in the Commands side of the application, and in the query handler, we need to accept a QueriesConnectionString. Good. Now, let's refactor the query handler. Defining a new SQL query, creating a connection string. I can copy this expression from here, and we can materialize the results of the query into DTO itself; no need to have an additional class here. Returning the list of students, good. With this modification, we can remove all this code, and this private class, too. Perfect. The query handler has become much simpler, thanks to the new schema designed specifically for its needs. By the way, if you need to see any of the intermediary code that I deleted during our refactoring process, just navigate to the commit history of this course's repository on GitHub. Most of the commits are segregated by modules, so you will be able to find all the intermediary code here as well. Alright, let's run the application, and you can see that the students are successfully displayed on the UI, and we can still filter them by the course name, and the number of courses. Note that the Read and Write databases are not synchronized yet. So if we try to update, say, Alice's personal info, it will not be displayed here, but hey, one step at a time.
Recap: Creating a Database for Queries

In the previous demo, you saw how we created a separate database for queries and re-targeted the query handler to work with it instead of the commands database. As a result, we were able to significantly simplify the query handler. As you can see, it now consists of just the SQL query itself, which we also simplified quite significantly, and it also consists of the single call to the database. We didn't even have to transform the data from the database using the private class as a temporary data storage. We rendered the results directly to the DTO. All this is a result of designing the new database in a way that perfectly aligns with the needs of our read model. As I mentioned previously, Read and Write models have different requirements, and it's impossible to come up with a single approach that would fit both of them. The only way we can fulfill the needs of both the Read and the Write models is if we apply different architectural approaches to them, and we did exactly that. Our database for commands complies with the third normal form, which is good for transaction processing. Whereas the database for reads is denormalized. The denormalization allowed us to minimize the number of joins and the amount of post-processing needed to get a given set of data. In fact, we didn't have any joins and any post-processing whatsoever. Note that it's a common phenomena in a lot of enterprise-level applications. High normal forms are good for commands and low normal forms are good for queries. We could go even further with the database for reads and introduce a different type of data storage, such as a document database. You need to look at what works best for the requirements you have for your Read model.
Scalability

Alright, these are all the first and the second benefits of CQRS, code simplicity and performance. How about scalability, the third one? Where is it in our code base? Now that we have separated our database into two data storages, it's easy to scale the Read side of the application. Just create as many databases for reads as you need, and make them synchronize with the database for writes. We will be talking about the synchronization strategies in the next module, but you need to implement this process only once. After that, you will be able to use it for as many databases as you would like. That's because of the immutable nature of the Read model. It doesn't mutate any state, it doesn't leave any side effects, and so it can be scaled pretty much indefinitely. It's not as simple to implement scaling within the command model, precisely because there's data mutation involved, but in the Read model, it is simple. Note that in our application, we are having just one Read model, but you could need more than one and thus, you could need to create more than one database with its own schema and synchronization mechanism. An example here is a mobile versus a web client. A mobile client usually doesn't need as much data as a web one, and it also could need it in another format. So it might make sense to create separate read models for each of them, and adjust those Read models to align perfectly with the clients' needs. Of course, if the needs of these two clients are not as different, then the single Read model would do just fine. So evaluate each different situation separately to see what makes the most sense in your particular case.
A Word of Caution Regarding the Database for Reads

A word of caution, though. You need to be prudent when applying software design patterns, including the CQRS pattern. There are costs and benefits attached to any design decision, and you need to carefully consider them before implementing it. Up to the point where you separate everything except the data storage, the maintainability costs of such a separation are not that high, and in most typical enterprise-level applications, the benefits will greatly overweight the costs. However, the situation changes when you introduce a new database to the mix, the database for Reads. The synchronization between the two databases introduces quite a lot of complexity. Eventual consistency between them, which we'll talk about in the next module, also introduces potential confusion for the users. For example, if you register a new student, and this student doesn't appear in the Read database right away, that might confuse your users, to the point when they might decide to register the student again and will end up having two same student records. You can mitigate this confusion by showing some information on the UI, for example, a message that the student is sent to the system and will be processed within a minute or two, but still, you need to be careful here. Eventual consistency, plus the additional effort of maintaining a separate database, add up to being quite significant costs, and in most cases, you would be just fine without a separate database for reads. The CQRS pattern can be just as effective with only a single database. Having that said, if you do need to scale your system, particularly the Read side of it, then a separate database can be a great solution for you.
Summary

In this module, we've introduced a separate database for Reads. This way, we have completed implementing the CQRS pattern. Now the Read and Write models are separated at each level; the API, the Application Services layer, the domain model, and the data storage. This separation allowed us to adjust the Read database perfectly for the needs of our query model, and thus simplify that model. And it also allowed us to scale the reads. Now we can have as many copies of the Read database as we need, and thus scale this part of our application pretty much indefinitely. We talked about what scalability is. This characteristic is similar to performance, but it's not the same thing. With performance optimizations, you are still bound to the resources of a single machine. Scalability means that you can utilize the resources of several and, potentially, an unlimited number of servers. There are plenty of examples of such a database level separations in the real world; indexed views, database replication, ElasticSearch are all examples of CQRS to some degree. You saw how we designed the database for reads. We have adjusted it to the needs of our Read model by denormalizing it. We had only one table in that database that contained all information about the students; their personal data, and both courses they are enrolled in. This allowed us to minimize the number of joins and the amount of post-processing we needed to get a given set of data. You will end up with this pattern in most applications; third normal relational form for the commands database, and the first form for the queries one. We discussed that you might want to have separate Read models, and by extension, separate read databases, for each client of your application, such as mobile and web. And we also discussed that you need to be cautious when introducing the second database. The additional costs associated with synchronization between the databases, and the potential confusion the eventual consistency introduces, might not be worth the benefits you get out of it. In many cases, a single database would be enough. Only introduce the second database if you truly need to scale your application. In the next module, we will talk about synchronizing the commands and queries databases. We will discuss different strategies of such a synchronization, and we will also talk about eventual consistency and the CAP theorem in the context of CQRS.
Synchronizing the Commands and Queries Databases
Introduction

Hi, my name is Vladimir Khorikov, and this is the CQRS in Practice course. In this module, we will talk about synchronizing the commands and queries databases. We will also discuss eventual consistency and the CAP theorem in the context of CQRS.
State-driven Projections

In the previous module, we introduced a new database for the Read part of our application. This new database is perfectly aligned with the needs of our queries. It is denormalized, and consists of only one table. The fact that there are now two separate databases means there should be a synchronization between them; such a synchronization is also called projection. So keep in mind that the words synchronization and projection are synonyms in the context of CQRS. So, let's talk about what strategies we could employ to implement such a projection. Those strategies can be divided into two categories, by the type of triggers that drive them. First, it is the projection driven by state, and second, the projection driven by events. The projection driven by state, in turn, can be divided into two sub-categories, synchronous and asynchronous. Let's take this first category, projections driven by state. It kind of sounds fancy, but it's really simple. The idea behind it is that you have a flag in each aggregate's table that tells you the aggregate has been updated since the last synchronization, and you need to synchronize it once again. In our sample project, the commands side of the application contains two aggregates, Student and Course. And so you wouldn't need to add four new columns, one for each table, but rather just two, one for the Student table, and the other one for the Course table. You can call it IsSyncRequired, IsDirty, or something similar. So, whenever a student gets updated, and that includes the data in the student, enrollment, and disenrollment tables; whenever they get updated, you raise the IsSyncRequired flag in the Student table. A separate synchronization process that runs at background then picks it up and either inserts a new row into the Read database, or updates an existing one. After that, it resets the flag. You can also add a separate small table, named Synchronization, with another flag in it called IsSyncRequired. This table will have just one row. Whenever any flag in the database is raised, the flag in that small table will be raised as well. The reason for this separate table is that the background synchronization process will be sampling changes in the commands database pretty often, and may potentially put a lot of pressure on the Student and the Course tables. The additional table will allow you to offload that pressure. Now the synchronization process will be sampling just this one table, instead of all students and courses. It will query the students and courses, only if the flag in the synchronization table is set to true. This projection mechanism is very straightforward, simple, and also easy to use. If you would ever need to rebuild the Read database, you just need to raise the flag for all records. And you can do that selectively, too, for just a subset of records. There are two ways to implement the commands part of this mechanism, the part that is responsible for raising the IsSyncRequired flags. First, create a database trigger. This trigger would monitor changes in all of your database tables, and update the flags accordingly. For example, if we enroll Alice in a new course, a new enrollment row will be inserted. The trigger can interpret this insertion as an update of the corresponding student, and can raise the flag for Alice's record, as well as the flag in the Synchronization table to let the background synchronization process do its job. Note that in order to handle the deletion, you would need to modify the way you handle unregistering of students. Instead of actually deleting them, you would need to implement a soft deletion with an IsDeleted flag. Then the deletion of a record will be treated the same way as a modification of it. There is a fully fledged example of this technique with state-driven asynchronous projections in my other course named, Domain-Driven Design: Working with Legacy Projects, in the module about Synchronizing Anticorruption Layer, so check it out for more details. The second way to raise a flag is to introduce it in the domain model itself explicitly, instead of relying on a database trigger. You can add two new IsSyncRequired properties to the domain model, one in the Student entity, and the other one in the course one, and update those properties whenever the client code modifies anything in those aggregates. That's pretty easy to do if your domain model is encapsulated. Here is an example. You can see that when we remove an enrollment, we raise the IsSyncRequired flag, too. And of course, to make this work, you would need to override the setters of the name and the email properties so that whenever the client code assigns a new value to them, the flag is raised as well. To raise the flag in the separate Synchronization table, you can use Event listeners in NHibernate or the Change Tracker in Entity Framework. This way, whenever you see an entity is being persisted with the IsSyncRequired flag, you will be able to automatically raise the flag in the Synchronization table, too. To see an example of it, refer to this article, section number seven. So, which one of these two ways to raise the flag should you choose? The one with the trigger or the one with the flags in the domain model? If you have control over the domain model, then go the explicit route, with the explicit flags in the domain model. It's always better to be explicit about what you do in the code. It tremendously improves the maintainability of your code base. Choose the approach with the database triggers, only if you don't have other choice; for example when you don't have control over your domain model.
Synchronous State-driven Projections

That was an example of an asynchronous state-driven projection strategy. The projection is driven by the state of the database, particularly the IsSyncRequired flags in it, and there is a separate background process that does the actual projection. It monitors those flags, and rebuilds the query database if it sees any changes. Let's now look at synchronous state-driven projection strategies. The difference between them, as you can guess from their names, is that one of them runs synchronously, and the other one asynchronously. So, what that means, exactly? The word asynchronous means without blocking the main worker. In the examples we discussed previously, the application is able to update a student in the database and continue working. A separate process then picks up this modification and does the projection. Our application doesn't wait for that separate process to complete this job, and so we can say that the projection is done asynchronously. So, how the synchronous version would look then? The synchronous version is when the application itself does all the work or waits until the background process completes the projection before returning an OK to the client. This approach is rarely applied when you have two separate databases for reads and writes, mainly because it increases the processing times quite dramatically. However, the benefit here is that the Read and the Write models are immediately consistent, which eliminates this confusion from the users when they don't see their changes immediately after submitting them. It might be very helpful if this is a strong requirement from your customer. I wouldn't recommend that you normally do that, though, at least not when there are two separate databases for commands and queries. Either implement the projection asynchronously, or keep the Read and Write models in the same database, to decrease the latency. Because of the additional latency, the synchronous projections don't scale well. That's because the latency will increase each time you add a new database copy for the queries. Your main application will have to update all the Read databases on its own, each time it modifies anything in the commands database, and the more databases you introduce for the reads, the more time it will take your application to update them all. There are two common examples of the state-driven projection strategies. Indexed views is an example of a synchronous projection. That's because all indexed views in relational databases are updated immediately, along with the tables they derive their data from. An example of an asynchronous projection is database replication. Database replicas, unlike the indexes views, are updated asynchronously.
Event-driven Projections

Another category of projections is event-driven projection. The main differentiating factor here is that event-driven projections are not driven by state, but instead they are driven by domain events. So, instead of the IsSyncRequired flags, and instead of the separate Synchronization table, the projection process will subscribe to the domain events the command side raises, and will use the information from those events to update the Read database. The benefit of this approach is that it scales really well, even better than the approach with asynchronous state-driven projections. That's because your application can just publish the domain events on a message bus, and you can have as many projection processes subscribing to those events as you want. They will not put additional pressure on the commands database. However, there's a significant drawback to this approach. There is no easy way to rebuild the Read database from scratch or re-run the projection in case of an error. In our commands database, the source of all data is the state, the current state of the students and the courses. We don't store the domain events that led to that state, and because it's impossible to derive the domain events from the latest state, your Read databases will become out of sync should they miss any domain event for some reason. You could mitigate this issue by storing the domain events along with the state in the commands database, but that would be quite a significant complexity overhead. And really, if you are going to store the domain events in the database anyway, then you should probably transition to an event-sourced architecture. Event sourcing is outside the scope of this course, though. So, the rule of thumb here is this. If you are implementing the CQRS pattern without event sourcing, that is, you don't store the domain events in your commands database, and just persist the latest state instead, then don't employ the event-driven projection approach. The state-driven projections would be a much better fit for you. On the other hand, if you are using event sourcing where the domain events are the first-class citizens in your database, then of course, use the event-driven projections. In this case, you will be able to rebuild the Read database by re-raising all the events in your commands database. In other words, align the projection strategies with the type of persistence mechanism you have chosen for your project.
Consistency

We discussed in the previous module that having two databases instead of one, introduces latency between the Write and the Read models. It brings in a potential confusion for the users, and you should carefully consider this drawback. For example, after registering a student, the user might not see the that new student immediately, decide that they did something wrong, register that student again, and end up with having two identical student records. If your customer strongly opposes such a user experience, then just go with the single database for reads and writes. You will still gain most of the benefits of the CQRS pattern. However, if your customer is not opposed to relaxing the consistency requirements, there are some ways to mitigate this potentially confusing experience. First of all, in many cases, there will be uniqueness constraints on most of the data in the commands database, and that will help avoid the duplicates. For example, there might be a requirement that all students' email addresses must be unique. In this case, even if the user doesn't see the new student right away, and try to register them again, the system will return an error saying that such student is already registered. Note that the consistency issues only involve the queries database. The commands database is always immediately consistent, and because you run the uniqueness constraint validations against the commands database, there will be no duplicates and the user will receive a proper error message. Let's actually elaborate on this point. People often ask, what if I need to query the database during a command execution, for example, to check for the email uniqueness constraint, and should I run a query from a command handler to do that? And the answer is, no. You shouldn't use a query for that, because the queries work with their own database, that might not be up to date with the commands database. In order to verify that the email is unique, you can query the data storage, but that should be the commands data storage, because it's always up to date. Just use a regular repository or the UnitOfWork class for that. So, you will still need to read the database from your commands. The difference between such reads and what the queries do is that the reads the commands perform are part of the command processing flow. The results of those reads don't cross the boundaries of the application. In other words, they are not intended for the user. Remember. after finishing the full separation between commands and queries in the previous module, we still had some read operations remaining in the repositories. That's the read operations I'm talking about; GetStudentById, GetCourseByName, and potentially GetStudentByEmail. They are intended to serve the commands, not queries. Note that it's a different story if you are implementing event sourcing. In this case, you are not able to efficiently query the current state from the commands database, because your database stores events, not the state, and so you will need to query the Read database in this case. You won't have any other choice, and you will have to deal with inconsistencies that flow from that. That's a whole other topic, though, and it should be addressed in another course. We will not be diving into it here.
Eventual Consistency

Alright, so uniqueness validations are one way to mitigate the issues with the consistency. Another way to do that is to train your users not to expect the data they see on the screen to be immediately consistent. I know, that sounds like cheating, and it looks like it just shifts the burden from lazy programmers to the users. After all, the software will become less usable if the users wouldn't see the results of their actions immediately, right? Actually, no. While it might be plausible to think that the users expect immediate consistency all the time, in reality, they don't. In fact, the whole concept of immediate consistency is counter-intuitive, because it's not how the things work in the real world. Think about it. When you move to a new home, do you expect your driver's license to reflect this change immediately? Of course, not. Your driver's license will show the old address until you go to a DMV, and ask them to issue a replacement. And during this quite long period of time, the information on your current driver's license will remain stale; it will remain obsolete. All your interactions with the real world are inherently asynchronous and involve eventual consistency, and the world around us is eventually consistent, too. Heck, even physics itself it eventually consistent. The information in the universe is passed with the speed of light, which is quite fast, but it's still not instantaneous. And so it doesn't take your users much effort to learn the concept of eventual consistency, because they already experience it in the real world, and in fact, it's the concept of immediate consistency that they learn when they start working with computers, not the other way around. You have probably already understood what eventual consistency stands for, but let me give an official definition. Eventual consistency is a consistency model, which guarantees that if no new updates are made to a given data item, eventually all accesses to that item will return the last updated value. So, in our example with the commands and queries databases, it means that if we modify a student, this change will eventually propagate to the Read database, and our users will be able see it on the UI. You can foster the user acceptance of the eventual consistency by displaying helpful messages and setting proper expectations. For example, when registering a student, show a text message saying that the student registration is submitted and will be processed shortly. This way the user will understand that this process takes time, and they shouldn't expect it to be completed immediately. Another way to handle this it to trick your users. Whenever they register a student, do update the student list, but do that locally, on the user machine only. By the time they refresh their screen again, this student will already be in the queries database, and the query result will contain the latest data. You can go even further and implement a two-way communication between the UI and the Read model. So, whenever the Read model receives an update, it will send a message to the UI with the updated search results. That would add some complexity, though, because you will need to keep the connection between the UI and the backend open, and it most cases, a simple message or the trick with the local data update would be enough. So, to summarize, if you decide to introduce a separate database for reads, then your query side will not be immediately consistent. It will be eventually consistent, which might or might not be acceptable, depending on the requirements your stakeholders pose to your application. There is a great article about how consistency is handled in the real world, called Starbucks Doesn't Use Two-Phase Commit. Check it out, here.
Versioning

There could be one area where the eventual consistency is problematic, and that is when you need to make a decision based on the current data, and the cost of making that decision based on the stale data is very high. This is almost never the case, though. One of the few exceptions here is high-frequency stock trading. In all other cases, you can mitigate this problem with versioning. You can keep a version of an aggregate in both the Read and Write databases, and then if you see that the user tries to update the stale record, show them an error. This is called optimistic concurrency control. Here's an example. Let's say that you are requesting a list of students from the Read database, and it returns you Alice's record with version one. Let's also say that someone updates Alice's personal information, and so the version of her record in the commands database becomes two. Then you also try to update her personal information based on the stale data from the queries database. In order to mitigate this issue, you just need to make this version number part of all the communications between the UI, the command, and the Read models. So, when the UI queries the Read model, it receives the version number, and includes it in the command that it sends to the Write model. The Write model can compare the version number in the command with the current version number of the aggregate and, if they are different, it can raise a concurrency error and reject the change. This way, you will inform the user that they need to refresh their screen to see the latest updates and include them into their new update request.
CQRS and the CAP Theorem

There is a close relationship between CQRS and the CAP theorem. The CAP theorem states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees: consistency, availability, and partition tolerance. Consistency means that every read receives the most recent write or an error. Availability, that every request receives a response, part from outages that affect all nodes in the system; and partition tolerance, that the system continues to operate despite messages being dropped, or delayed, between the network nodes. For example, if you choose availability and consistency, that would be a typical relational database server running on a single node. All read requests to this server always get the latest version of the data, and this server is always available. The drawback here is that there is no partition tolerance, meaning that you cannot have more than one server working for your application. If you choose consistency and partition tolerance, that would be an anti-pattern called distributed monolith. All reads to such a system will still be fully consistent with the writes, but because you also chose partition tolerance, there are now multiple servers, and to ensure the consistency among all of them, you will need to block the whole system, and wait until each change propagates to every server. This would make the system unresponsive and unavailable for the period of time during which it waits for the propagation to complete. Imagine what would happen if when you post a picture on Facebook, the whole website would stop working to ensure that your friends' newsfeed is consistent with your latest update. And finally, if you choose availability and partition tolerance, then you do that at the expense of consistency, which means that if there is a connectivity loss between two servers, and you allow them to work independently, then they can go out of sync with each other really quickly. So, again, you can pick two out of the three guarantees, but you can never pick all three, which means that you need to find a proper balance for your particular situation. CQRS is great, because it allows you to make different choices for reads and writes. The trick here is to make the trade-off between partitioning and consistency differently on the Read and Write sides. In most cases, you can give full consistency on the commands side, and give up a level of partition tolerance. That's the typical relation database server scenario. You can do that as long as all your command handlers can operate on a single machine. At the same time, you can give up the full consistency on the queries side in order to obtain partitionability. You want to ensure partitionability on the Read side, because you want to scale the reads up to several machines to enable scalability. And you want to ensure consistency on the Write side, to make sure all changes are consistent, at the expense of having only one machine that handles the commands.
Summary

In this module, we discussed the synchronization between the commands and queries databases. Such synchronization is also called projection. You learned about the two projection strategies, using state versus using events as the drivers for it. State-driven projection is when you introduce an IsSyncRequired flag in the aggregates' tables, and raise it whenever there's a change to them. You can use this flag using database triggers, or explicitly in your domain model. The explicit route is the better choice in most cases. The state-driven projections can be synchronous and asynchronous. Synchronous projections don't scale well, so choose the asynchronous option by default. We also discussed event-driven projections. That is, when you use the domain events raised by the commands side to build the queries database. The rule of thumb here is this. If you are implementing the CQRS pattern without event-sourcing, that is, if you persist the state, then use the state-driven projection. If you persist events instead of state, then choose the event-driven one. We talked about immediate and eventual consistency. Although you might be used to immediate consistency in the world of relational databases, that's not how the real world operates, and people usually don't have trouble dealing with eventual consistency in the software, too; just set proper expectations, and implement data versioning to enable the optimistic concurrency control. Finally, we discussed the CAP theorem in the context of CQRS. Remember, CQRS is about making different choices for different situations, particularly for Read and Write sides of your application. And that is also true when it comes to finding the balance within the CAP theorem. Choose consistency and availability at the expense of partitioning for writes, and choose availability and partitioning at the expense of consistency for reads. In the next module, we will talk about CQRS Best Practices and Common Misconceptions.
CQRS Best Practices and Misconceptions
Introduction

Hi, my name is Vladimir Khorikov and this is the CQRS in Practice course. In this module, we will talk about some of the best practices and common misconceptions around the CQRS pattern.
CQRS and Event Sourcing

We have come a long way refactoring our application to follow the CQRS pattern. Let's reiterate some of the best practices and misconceptions we encountered along the way, as well as discuss some that we haven't yet talked about. The first one is the relation between CQRS and Event Sourcing. CQRS is often described as a stepping stone to Event Sourcing. In other words, it's described as just a transition stage that eventually leads to the Event Sourced architecture. It is not the case. CQRS can be applied on its own, and may provide a lot of benefits to the project, even if you never implement Event Sourcing. Even more, Event Sourcing brings a lot of complexity to the table, and the bar for it is much higher than it is for CQRS; even higher than for the separation at the database level that we discussed in the two previous modules. Having that said, there is still a lot of value in event sourced systems, too; it's just the complexity overhead is not as often justified as for CQRS. Usually, the applications that benefit from Event Sourcing, despite all that complexity, are a special kind of system. Those for which it is essential to keep track of the events that led to a particular state. For example, pretty much everything in the finance tech area would benefit from Event Sourcing. In such systems, you must provide an audit log, a trail of all the financial transactions, and so treating domain events as the first-class citizens makes a lot of sense here. So, CQRS can be applied on its own with great success, independently from Event Sourcing. These two concepts are mostly orthogonal to each other, mostly, but not completely. You don't need Event Sourcing to implement CQRS, but you do usually need CQRS to implement Event Sourcing. Event Sourcing without CQRS, without separating reads and writes, is a less common and less scalable solution. In theory, you could do that, but I've yet to see it being implemented in practice.
Evolutionary Design

We've discussed the evolutionary approach previously in the course, but let's reiterate it once again as it's quite an important concept. You don't have to implement all the techniques from this course. Do not just throw every related CQRS concept at your application. Defer these decisions until you have proven their need. And by the way, that's the approach I recommend you take with any design pattern, not just CQRS. Remember, every design pattern has both costs and benefits attached to it, and you need to be sure that the benefits outweigh the costs before applying that pattern. Again, you don't need to apply all the techniques at once. Even with just the separation at the API level, with the task-based API alone, which we implemented first in this course, you will reap significant benefits. Note that CQRS is not an application-wide pattern, like the Microservices architecture. Rather, it should be bound to a specific bounded context or microservice. It means that you can make different decisions in different bounded contexts when it comes to CQRS. Say, implement the full separation down to the data level in one bounded context, because it is very important to the business, and has high performance requirements. And at the same time, implement just the separation at the API level in another, not that important bounded context; or even keep spaghetti code with giant CRUD-oriented endpoints. That could be an option, too, depending on your project's circumstances. However, in the vast majority of enterprise-level applications, the sweet spot is the separation down to the domain model level. The separation of everything, except the database. That usually allows you to achieve the best balance between the costs and the benefits.
Using Commands and Queries from Handlers

There is another question that we discussed throughout this course and which merits reiteration, and that is, can you use other commands and queries from your command and query handlers? The short answer is, no, but the long answer actually differs, depending on the situation. There are quite a few of permutations in this question, so let's tackle them one by one. First, can you use another command from a command handler? Here is the example I brought up in an earlier module, with disenrolling the student from the existing courses before unregistering them. You can see we are instantiating two disenroll commands, and dispatching them using the message dispatcher. It's tempting to do that, but it is contrary to the very notion of commands. It is the client that trigger commands, not our system. Our system reacts to those commands and produces domain events. It cannot create subsequent commands on its own. If you need to reuse some code between command handlers, extract it into a separate class, such as a domain service. Don't reuse command handlers themselves. Here's a picture that will help clarify the relationships between our application, the client, and the types of messages they exchange with each other. As you can see, the client sees the current state of the application and produces a command. The Write model receives the command and produces events. The other side of our application, the Read side, receives those events and uses them to build up the snapshot of the current state. That state is what is then shown to the client. This is the circulation of the messages and the state in the application ecosystem. A command is a representation of what the clients can do with the application, and only the client can raise them. An event is a representation of what has happened to the application. And finally, the state is the sum of all the events we've had up to this moment. Next, can you use a query from a command handler? Here, the answer depends on whether or not you can have a reliable way to request the current state in the command side. If you do, then no need to use a query, just refer to the command database directly to retrieve the piece of data the command needs. If there is no reliable way to request the current state, then you don't have any choice other than using a query from a command handler. For example, if you have implemented Event Sourcing, then the commands cannot request the current state from its own database with a reasonable performance, because rebuilding that state out of events every time you need to know something is not practical. So, you need to use the Read database, and you need to do that via a query. Note that in this case, there is a whole set of issues that come into play, because the Read and the Write databases are not immediately consistent. For example, unique constraint validations might not be done because of that. Note that they only come into play, if you are implementing Event Sourcing, and that's a story for another day. Finally, can you use a command or a query from a query handler? Well, the answer for commands is obviously, no, because a query handler cannot mutate state. As for the use of another query from a query handler, the guidelines here are not as strict, because the Read model is immutable, and it's hard to mess up with it, thanks to this fact. But in general, I would say don't reuse the query handlers, either. Extract the common code out to some service, and use that service instead.
One-way Commands

Another commonly held belief is that commands should always be unidirectional. In other words, that the commands should not return anything, because that would violate the underlying CQS principle, the Command-Query Separation Principle. The approach that is suggested instead is to poll an external source for the command execution result. So, when you, say, register a student, the command side shouldn't do that right away; it shouldn't provide you with a result synchronously, but rather it should start executing the request asynchronously, and you need to poll a separate API endpoint to know the results. That is another misconception. Truly one-way commands are impossible; they always need to return some kind of acknowledgment; either an OK, an error, or a locator, which you can use to poll the results from a separate API endpoint. And so, why return this locator if you can just return the results right away? If the operation is not inherently asynchronous, and doesn't take much time to complete, complete it synchronously, and return an OK or a validation error. If it is an update of some entity, you can also return the aggregate's new version number. No reason to implement an overcomplicated flow of events where a simple synchronous execution would do just fine. It's also fine to return an id of the newly created entity. This one looks like a true violation of the CQS principle. After all, it looks like the command returns a genuine piece of data here, but no, it's not that. It's okay for commands to return a locator of the resources they create, and if you are creating the resource synchronously, then the id of that resource would be this locator.
CQRS vs. the Specification Pattern

Alright, this is a big one. There is a controversy between two Domain-Driven Design patterns, CQRS and the Specification pattern. By the way, I have a course about the Specification pattern, too. I highly recommend that you check it out. So the controversy between them is that these two patterns are incompatible. The Specification pattern allows us to encapsulate a piece of domain knowledge into a single place, and then reuse it in three scenarios; data retrieval, user input validation, and creation of a new object. We are currently interested in only two of these three scenarios. This is useful because it allows you to avoid the domain knowledge duplication. The same domain class here would be used for both validation of the incoming data and filtration of the data in the database before showing it to the user. At the same time, the CQRS pattern proposes the separation of the two. Validation belongs to the command side, because it precedes data mutation. You validate data before you change something. Whereas data retrieval belongs to the Read side; it's what the client queries to see what data there is in our system. And so there is a clear contradiction here. On the one hand, the Specification pattern advocates for having a single domain model for these two concerns. On the other, CQRS advocates for splitting the domain model into two, and dealing with those concerns separately. So, which one is that? That's a classic example of the contradiction between the DRY principle, which stands for Don't Repeat Yourself, and the principle of loose coupling, and the guideline here is this. The loose coupling wins in the vast majority of cases, except for the simplest ones. Domain knowledge duplication is a lesser evil than the high coupling between the system components. When you duplicate the domain knowledge, it's not very convenient, but it's not that bad when you compare it to the alternative. With the high coupling, you are not able to do anything, because your hands are tied by the restrictions posed to another component. And you saw this in our sample project, too. Before we removed the domain model from the Read side, we just couldn't implement an efficient querying mechanism, all because of the unified model for commands and queries. The Specification pattern works well in simpler cases. However, in large systems, you almost always would prefer loose coupling over preventing the domain knowledge duplication between the reads and writes.
Resource List

Here are some useful links that I referred to in this course. The source code for this course is available on GitHub. It contains two folders with the initial and the final versions of our application. You can also look at the commit history to see what changes were done in each module. If you need an introductory course on the topic of Domain-Driven Design, I recommend my Domain-Driven Design in Practice, one. This is the CQRS book written by Greg Young. This is the article where you can see how to use event listeners in NHibernate, and the Change Tracker in Entity Framework to automatically raise the IsSyncRequired flag in a separate table, whenever you update an entity. You need Section 7 in that article. And this is the article about how consistency is handled in the real world, using the Starbucks example.
Course Summary

We've made great progress in this course. You have learned about the CQRS pattern and how to apply it in practice. The three goals of CQRS are simplicity, performance, and scalability; simplicity being the most important one. We gradually introduced more and more separation to our system. First, we separated our API endpoints, and split the single giant update method into several task-based ones. That allowed us to transition away from the CRUD-based thinking, and introduce a task-based interface, both the UI and the API. Next, we extracted explicit commands and queries, and handlers for them out of our controller. That allowed us to introduce a unified. NET interface for the handlers, and then create decorators to tackle cross-cutting concerns with the minimal amount of effort. After that, we extracted the domain model out of the Read side, which allowed us to both simplify the domain model and write high-performance SQL queries for the Read side. Next, we introduced the separation at the data level. We created a separate database for reads with the schema perfectly aligned with the needs of the Read model. That allowed us to improve the performance, simplify the Read model even further, and enable scalability on the query side. Finally, we discussed the synchronization between the Read and Write databases, and we also discussed best practices and common misconceptions around the CQRS pattern. Be sure to subscribe to my blog, where I will be putting announcements about more courses about Domain-Driven Design. This is the short link for you to do that. Also, feel free to get in touch with me. Here are my email, Twitter handler, and blog. This is Vladimir Khorikov. Thank you for listening. 